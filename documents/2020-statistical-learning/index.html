<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Statistical Learning Notes (ISLR)</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Statistical Learning Notes (ISLR)</h1>
</header>
<ul>
<li><a href="#introduction">Introduction</a>
<ul>
<li><a href="#a-brief-history-of-statistical-learning">A Brief History of Statistical Learning</a></li>
<li><a href="#misc-concepts">Misc Concepts</a>
<ul>
<li><a href="#box-plots">Box-plots</a></li>
</ul></li>
</ul></li>
<li><a href="#statistical-learning">Statistical Learning</a>
<ul>
<li><a href="#what-is-statistical-learning">What is Statistical Learning</a>
<ul>
<li><a href="#why-estimate-f">Why Estimate <span class="math inline">\(f\)</span>?</a></li>
<li><a href="#prediction">Prediction</a></li>
<li><a href="#inference">Inference</a></li>
<li><a href="#how-do-we-estimate-f">How Do We Estimate <span class="math inline">\(f\)</span>?</a></li>
<li><a href="#parametric-methods">Parametric Methods</a></li>
<li><a href="#non-parametric-methods">Non-parametric Methods</a></li>
<li><a href="#the-trade-off-between-prediction-accuracy-and-model-interpretability">The Trade-Off Between Prediction Accuracy and Model Interpretability</a></li>
<li><a href="#supervised-versus-unsupervised-learning">Supervised Versus Unsupervised Learning</a></li>
<li><a href="#regression-versus-classification-problems">Regression Versus Classification Problems</a></li>
</ul></li>
<li><a href="#assessing-model-accuracy">Assessing Model Accuracy</a>
<ul>
<li><a href="#measuring-the-quality-of-fit">Measuring the Quality of Fit</a></li>
<li><a href="#the-bias-variance-trade-off">The Bias-Variance Trade-Off</a></li>
<li><a href="#the-classification-setting">The Classification Setting</a></li>
<li><a href="#the-bayes-classifier">The Bayes Classifier</a></li>
<li><a href="#k-nearest-neighbors">K-Nearest Neighbors</a></li>
</ul></li>
</ul></li>
<li><a href="#linear-regression">Linear Regression</a>
<ul>
<li><a href="#simple-linear-regression">Simple Linear Regression</a>
<ul>
<li><a href="#estimating-the-coefficients">Estimating the Coefficients</a></li>
<li><a href="#assessing-the-accuracy-of-the-coefficient-estimates">Assessing the Accuracy of the Coefficient Estimates</a></li>
<li><a href="#assessing-the-accuracy-of-the-model">Assessing the Accuracy of the Model</a></li>
<li><a href="#residual-error">Residual Error</a></li>
<li><a href="#r2-statistic"><span class="math inline">\(R^2\)</span> Statistic</a></li>
</ul></li>
<li><a href="#multiple-linear-regression">Multiple Linear Regression</a>
<ul>
<li><a href="#estimating-the-regression-coefficients">Estimating the Regression Coefficients</a></li>
<li><a href="#some-important-questions">Some Important Questions</a></li>
<li><a href="#is-there-a-relationship-between-the-response-and-predictors">Is There a Relationship Between the Response and Predictors?</a></li>
<li><a href="#deciding-on-important-variables">Deciding on Important Variables</a></li>
<li><a href="#model-fit">Model Fit</a></li>
<li><a href="#predictions">Predictions</a></li>
</ul></li>
<li><a href="#other-considerations-in-the-regression-model">Other Considerations in the Regression Model</a>
<ul>
<li><a href="#qualitative-predictors">Qualitative Predictors</a></li>
<li><a href="#predictors-with-only-two-levels">Predictors with Only Two Levels</a></li>
<li><a href="#qualitative-predictors-with-more-than-two-levels">Qualitative Predictors with More than Two Levels</a></li>
<li><a href="#extensions-of-the-linear-model">Extensions of the Linear Model</a></li>
<li><a href="#removing-the-additive-assumption">Removing the Additive Assumption</a></li>
<li><a href="#non-linear-relationships">Non-linear Relationships</a></li>
<li><a href="#potential-problems-and-troubleshooting">Potential Problems and Troubleshooting</a></li>
</ul></li>
</ul></li>
</ul>
<h1 id="introduction">Introduction</h1>
<h2 id="a-brief-history-of-statistical-learning">A Brief History of Statistical Learning</h2>
<ol type="1">
<li><p>At the beginning of the nineteenth century, Legendre and Gauss published papers on the method of least squares, which implemented the earliest form of what is now known as linear regression.</p></li>
<li><p>In 1936 Fisher proposed linear discriminant analysis.</p></li>
<li><p>In the 1940s, various authors put forth an alternative approach, logistic regression.</p></li>
<li><p>In the 1950’s, Frank Rosenblatt introduced the Perceptron and Neural Networks.</p></li>
<li><p>In the 1960’s, various authors introduced Nearest Neighbor and K-means clustering.</p></li>
<li><p>In the early 1970s, Nelder and Wedderburn coined the term generalized linear models for an entire class of statistical learning methods that include both linear and logistic regression as special cases.</p></li>
<li><p>By the end of the 1970s, many more techniques for learning from data were available but all were almost all linear because of computational limitations.</p></li>
<li><p>By the 1980s, computing technology had finally improved sufficiently that non-linear methods were no longer computationally prohibitive.</p></li>
<li><p>In the mid 1980s, Breiman, Friedman, Olshen and Stone introduced classification and regression trees, including cross-validation for model selection.</p></li>
<li><p>In 1986, Hastie and Tibshirani introduced generalized additive models for a class of non-linear extensions to generalized linear models.</p></li>
<li><p>In the 1990’s, Vapnik introduced Support Vector Machines.</p></li>
<li><p>In the 2000’s, Brieman introduced Random Forest</p></li>
<li><p>In the 2000’s, Hinton popularized Deep Learning and Artificial Neural Networks.</p></li>
</ol>
<h2 id="misc-concepts">Misc Concepts</h2>
<h3 id="box-plots">Box-plots</h3>
<p>One should also look at box-plots for each individual features. The box plot displays the distribution of data based on the five number summary: minimum, first quartile, median, third quartile, and maximum.</p>
<p>We can look for outliers beyond 3*IQR (Inter Quartile Range). Bootstrapping the data set also provides some insight on outliers.</p>
<figure>
<img src="box-plot.png" style="width:6cm" alt="" /><figcaption>image</figcaption>
</figure>
<h1 id="statistical-learning">Statistical Learning</h1>
<h2 id="what-is-statistical-learning">What is Statistical Learning</h2>
<p><span class="math inline">\(n\)</span> – number of observations or training data.</p>
<p><span class="math inline">\(p\)</span> – number of features or parameters.</p>
<p><span class="math inline">\(x_{ij}\)</span> – the value of the <span class="math inline">\(j\)</span>th predictor, or input, for observation <span class="math inline">\(i\)</span></p>
<p><span class="math inline">\(X = (x_1, \dots, x_p)\)</span> – input vector a.k.a features, predictors, independent variables.</p>
<p><span class="math inline">\(\epsilon\)</span> – error term, independent of <span class="math inline">\(X\)</span> and has mean zero.</p>
<p><span class="math inline">\(Y = f(X) + \epsilon\)</span> – a model a.k.a. output or dependent variable.</p>
<p><span class="math inline">\(y_i\)</span> – the response variable for the <span class="math inline">\(i\)</span>th observation</p>
<p>In essence, statistical learning refers to a set of approaches for estimating <span class="math inline">\(f\)</span>.</p>
<h3 id="why-estimate-f">Why Estimate <span class="math inline">\(f\)</span>?</h3>
<p>There are two main reasons for estimating <span class="math inline">\(f\)</span>: prediction and inference.</p>
<h3 class="unnumbered" id="prediction">Prediction</h3>
<p>We can predict <span class="math inline">\(Y\)</span> using <span class="math inline">\(\hat Y = \hat f(X)\)</span> where <span class="math inline">\(\hat f\)</span> represents an estimate of <span class="math inline">\(f\)</span> and <span class="math inline">\(\hat Y\)</span> represents the resulting prediction for <span class="math inline">\(Y\)</span>. The accuracy of <span class="math inline">\(\hat Y\)</span> as a prediction for <span class="math inline">\(Y\)</span> depends on two quantities, the reducible error and the irreducible error.</p>
<p><em>Reducible error</em> is the result of an inaccurate statistical learning technique.</p>
<p><em>Irreducible error</em> occurs because <span class="math inline">\(Y\)</span> is a function of <span class="math inline">\(\epsilon\)</span> which has variability that is dependent of <span class="math inline">\(X\)</span>, so cannot be reduced via a statistical learning technique. Irreducible error may be larger than zero because of unmeasured variables or unmeasurable variation. The irreducible error will always provide an upper bound on the accuracy of our prediction for Y and is almost always unknown in practice.</p>
<p><span class="math display">\[E[(Y - \hat f (x))^2 | X = x] = \underbrace{[f(x) - \hat f(x)]^2}_{\text{Reducible}} + \underbrace{Var(\epsilon)}_{\text{Irreducible}}\]</span></p>
<p>Note, <span class="math inline">\(E(Y - \hat Y)^2 = E[(Y - \hat f (x))^2 | X = x] = E[f(X) + \epsilon - \hat f(X)]^2\)</span> represents the average, or expected value, of the squared difference between the predicted and actual value of <span class="math inline">\(Y\)</span> and <span class="math inline">\(Var(\epsilon)\)</span> represents the variance associated with the error term <span class="math inline">\(\epsilon\)</span>. We square the values in order to ignore the resulting sign when finding averages.</p>
<h3 class="unnumbered" id="inference">Inference</h3>
<p>Often, we are interested in understanding how <span class="math inline">\(Y\)</span> changes as a function of <span class="math inline">\(X_1,\dots,X_p\)</span>. That is, when we vary the value of a given feature, what should the result look like. Now <span class="math inline">\(\hat f\)</span> cannot be treated as a black box, because we need to know its exact form.</p>
<p>Common questions occurring in this setting include:</p>
<ul>
<li><p>Which predictors are associated with the response?</p></li>
<li><p>What is the relationship between the response and each predictor?</p></li>
<li><p>Can the relationship between Y and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?</p></li>
</ul>
<p>Depending on whether our ultimate goal is prediction, inference, or a combination of the two, different methods for estimating <span class="math inline">\(f\)</span> may be appropriate. Linear models allow for relatively simple and interpretable inference, but may not yield as accurate predictions as some other approaches. In contrast, some highly non-linear approaches can potentially provide quite accurate predictions for <span class="math inline">\(Y\)</span> at the expense of a less interpretable model, which makes inference more challenging.</p>
<h3 id="how-do-we-estimate-f">How Do We Estimate <span class="math inline">\(f\)</span>?</h3>
<p>Our training data or observations can be represented as <span class="math inline">\({(x_1, y_1),(x_2, y_2),...,(x_n, y_n)}\)</span> where <span class="math inline">\(x_i = (x_{i_1}, x_{i_2},...,x_{i_p})^T\)</span> are vectors and <span class="math inline">\(y_i\)</span> are typically scalars.</p>
<p>Then, we want to find a function <span class="math inline">\(\hat f\)</span> such that <span class="math inline">\(Y \approx \hat f(X)\)</span> for any observation <span class="math inline">\((X, Y)\)</span>. Broadly speaking, most statistical learning methods for this task can be characterized as either <em>parametric</em> or <em>non-parametric</em>.</p>
<h3 class="unnumbered" id="parametric-methods">Parametric Methods</h3>
<p>This approach reduces the problem of estimating <span class="math inline">\(f\)</span> down to one of estimating a set of parameters.</p>
<ol type="1">
<li><p>First, we make an assumption about the functional form, or shape, of <span class="math inline">\(f\)</span>.</p></li>
<li><p>After a model has been selected, we need a procedure that uses the training data to fit or train the model.</p></li>
</ol>
<p>For example, if we assume <span class="math inline">\(f\)</span> is linear, then <span class="math inline">\(f(X) = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_p X_p\)</span>. To train the linear model, we need to estimate the parameters <span class="math inline">\(\beta_0, \beta_1,..., \beta_p\)</span>, which is commonly done using (ordinary) least squares.</p>
<p>In general, fitting a more flexible model requires estimating a greater number of parameters. These more complex models can lead to a phenomenon known as <strong>overfitting</strong> the data, which essentially means they follow the errors, or noise, too closely.</p>
<h3 class="unnumbered" id="non-parametric-methods">Non-parametric Methods</h3>
<p>Non-parametric methods do not make explicit assumptions about the functional form of <span class="math inline">\(f\)</span> and instead seek an estimate of <span class="math inline">\(f\)</span> that gets as close to the data points as possible while being reasonably smooth.</p>
<p>A very large number of observations (far more than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for <span class="math inline">\(f\)</span>. In order to fit a thin-plate spline, the data analyst must select a level of smoothness.</p>
<h3 id="the-trade-off-between-prediction-accuracy-and-model-interpretability">The Trade-Off Between Prediction Accuracy and Model Interpretability</h3>
<figure>
<img src="isl-figure-1.png" style="width:11cm" alt="" /><figcaption>image</figcaption>
</figure>
<p>If we are mainly interested in inference, then restrictive models are much more interpretable. In contrast, very flexible approaches, such as the splines and the boosting methods can lead to such complicated estimates of <span class="math inline">\(f\)</span> that it is difficult to understand how any individual predictor is associated with the response. Though there are clear advantages to using simple and relatively inflexible statistical learning methods when inference is the goal, the converse is not typically true. Instead we will often obtain more accurate predictions using a less flexible method. This phenomenon has to do with the potential for overfitting in highly flexible methods.</p>
<h3 id="supervised-versus-unsupervised-learning">Supervised Versus Unsupervised Learning</h3>
<p>Most statistical learning problems fall into one of two categories: supervised or unsupervised.</p>
<p>In <em>supervised learning</em>, for each observation of the predictor measurement(s) <span class="math inline">\(x_i\)</span>, for <span class="math inline">\(i = 1,...,n\)</span>, there is an associated response measurement <span class="math inline">\(y_i\)</span>. We wish to fit a model that relates the response to the predictors, with the aim of accurately predicting the response for future observations (prediction) or better understanding the relationship between the response and the predictors (inference).</p>
<p>In contrast, <em>unsupervised learning</em> describes the situation in which for every observation <span class="math inline">\(i = 1,...,n\)</span>, we observe a vector of measurements <span class="math inline">\(x_i\)</span> but no associated response <span class="math inline">\(y_i\)</span>. It is not possible to fit a linear regression model, since there is no response variable to predict. One statistical learning tool that we may use in this setting is cluster analysis which aims to ascertain, on the basis of <span class="math inline">\(x_1,..., x_n\)</span>, whether the observations fall into relatively distinct groups.</p>
<p>In a semi-supervised learning problem, we wish to use a statistical learning method that can incorporate the <span class="math inline">\(m\)</span> observations for which response measurements are available as well as the <span class="math inline">\(n - m\)</span> observations for which they are not.</p>
<h3 id="regression-versus-classification-problems">Regression Versus Classification Problems</h3>
<p>Variables can be characterized as either quantitative (taking on numerical values) or qualitative/categorical. We tend to refer to problems with a quantitative response as regression problems, while those involving a qualitative response are often referred to as classification problems, but it’s not always clear-cut and many problems can use either responses. Whether the predictors are qualitative or quantitative is generally considered less important provided that any qualitative predictors are properly coded before the analysis is performed.</p>
<h2 id="assessing-model-accuracy">Assessing Model Accuracy</h2>
<h3 id="measuring-the-quality-of-fit">Measuring the Quality of Fit</h3>
<p>In order to evaluate the performance of a statistical learning method on a given data set, we need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation.</p>
<p>The most commonly-used measure is the <em>mean squared error</em> (MSE), given by <span class="math display">\[MSE = \frac{1}{n} \sum_{i=1}^n (y_i - \hat f(x_i))^2.\]</span></p>
<p>The MSE computed using the training data that was used to fit the model can be referred to as the training MSE. We want to choose the method that gives the lowest test MSE, i.e. we want <span class="math inline">\(\hat f(x_0)\)</span> to be approximately equal to <span class="math inline">\(y_0\)</span>, where <span class="math inline">\((x_0, y_0)\)</span> is a previously unseen test observation not used to train the statistical learning method.</p>
<p>As model flexibility increases, training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. This is a fundamental property of statistical learning that holds regardless of the particular data set at hand and regardless of the statistical method being used. When we overfit the training data, the test MSE will be very large because the supposed patterns that the method found in the training data simply don’t exist in the test data. One important method for estimating test MSE using the training data is cross-validation, examined later.</p>
<h3 id="the-bias-variance-trade-off">The Bias-Variance Trade-Off</h3>
<p>The expected test MSE, for a given value <span class="math inline">\(x_0\)</span>, can always be decomposed into the sum of three fundamental quantities: the variance of <span class="math inline">\(\hat f(x_0)\)</span>, the squared bias of <span class="math inline">\(\hat f(x_0)\)</span> and the variance of the error terms. That is,</p>
<p><span class="math display">\[E \Big (y_0 - \hat f(x_0) \Big )^2 = Var( \hat f(x_0)) + [Bias( \hat f(x_0))]^2 + Var(\epsilon).\]</span></p>
<p>Here, <span class="math inline">\(E \Big (y_0 - \hat f(x_0) \Big )^2\)</span> defines the expected test MSE, computed by averaging the term over all possible values of <span class="math inline">\(x_0\)</span> in the test set. In order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves low variance and low bias.</p>
<p><em>Variance</em> refers to the amount by which <span class="math inline">\(\hat f\)</span> would change if we estimated it using a different training data set. In general, more flexible statistical methods have higher variance. This is because changing any one of the data points may cause the estimate <span class="math inline">\(\hat f\)</span> to change considerably. A model with high variance does not generalize on the data which it hasn’t seen before and is said to be overfitting.</p>
<p><em>Bias</em> refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. Generally, more flexible methods result in less bias. A model with high bias will be making overly strong assumptions about the training data and is said to be underfitting.</p>
<figure>
<img src="bias-variance-tradeoff.png" style="width:13cm" alt="" /><figcaption>image</figcaption>
</figure>
<figure>
<img src="bias-variance-tuning.jpg" style="width:7cm" alt="" /><figcaption>image</figcaption>
</figure>
<p>The relationship between bias, variance, and test set MSE is referred to as the <em>bias-variance trade-off</em>.</p>
<p>As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. The relative rate of change of these two quantities determines whether the test MSE increases or decreases. As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the expected test MSE declines. However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases.</p>
<h3 id="the-classification-setting">The Classification Setting</h3>
<p>Many of the previous concepts, including the bias-variance trade-off, transfer over to the classification setting with only some modifications due to the fact that <span class="math inline">\(y_i\)</span> is no longer numerical. Suppose that we seek to estimate <span class="math inline">\(f\)</span> on the basis of training observations <span class="math inline">\(\{(x_1, y_1),...,(x_n, y_n)\}\)</span>, where now <span class="math inline">\(y_1,...,y_n\)</span> are qualitative.</p>
<p>The most common approach for quantifying the accuracy of our estimate <span class="math inline">\(\hat f\)</span> is the <em>training error rate</em>, the proportion of mistakes that are made if we apply our estimated <span class="math inline">\(\hat f\)</span> to the training observations:</p>
<p><span class="math display">\[\frac{1}{n} \sum_{i=1}^n I(y_i  \neq \hat y_i)\]</span></p>
<p>Here <span class="math inline">\(\hat y_i\)</span> is the predicted class label for the <span class="math inline">\(i\)</span>th observation using <span class="math inline">\(\hat f\)</span>, and <span class="math inline">\(I(y_i \neq \hat y_i)\)</span> is an indicator variable that equals 1 if the <span class="math inline">\(i\)</span>th observation was misclassified, i.e. <span class="math inline">\(y_i \neq \hat y_i\)</span>, and equals zero if it was was classified correctly, i.e. <span class="math inline">\(y_i = \hat y_i\)</span>.</p>
<p>The <em>test error rate</em> associated with a set of test observations of the form <span class="math inline">\((x_0, y_0)\)</span> is given by, <span class="math display">\[Ave(I(y_0 \neq \hat y_0)),\]</span> where <span class="math inline">\(\hat y_0\)</span> is the predicted class label. A good classifier minimizes the test error.</p>
<h3 id="the-bayes-classifier">The Bayes Classifier</h3>
<p>To minimize the test error rate, on average, we should simply assign a test observation with predictor vector <span class="math inline">\(x_0\)</span> to the class <span class="math inline">\(j\)</span> for which the conditional probability <span class="math display">\[\Pr(Y = j|X = x_0)\]</span> is largest. This very simple classifier is called the <em>Bayes classifier</em>. In a two-class problem the classification will be class one if <span class="math inline">\(Pr(Y = 1|X = x_0) &gt; 0.5\)</span>, and class two otherwise. The line where the probability is exactly 0.5 is called the <em>Bayes decision boundary</em>.</p>
<p>The Bayes classifier produces the lowest possible test error rate, called <em>the Bayes error rate</em>, given by, <span class="math display">\[1 - E \Bigg( \max_j \Pr(Y = j|X) \Bigg ),\]</span> where the expectation averages the probability over all possible values of <span class="math inline">\(X\)</span>. The Bayes error rate is analogous to the irreducible error.</p>
<p>In theory we would always like to predict qualitative responses using the Bayes classifier. But for real data, we do not know the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, and so computing the Bayes classifier is impossible. Therefore, the Bayes classifier serves as an unattainable gold standard against which to compare other methods.</p>
<h3 id="k-nearest-neighbors">K-Nearest Neighbors</h3>
<p>Given a positive integer <span class="math inline">\(K\)</span> and a test observation <span class="math inline">\(x_0\)</span>, the <em>K-nearest neighbors</em> (KNN) classifier first identifies the <span class="math inline">\(K\)</span> points in the training data that are closest to <span class="math inline">\(x_0\)</span>, represented by <span class="math inline">\(\mathcal N_0\)</span>. It then estimates the conditional probability for class <span class="math inline">\(j\)</span> as the fraction of points in <span class="math inline">\(\mathcal N_0\)</span> whose response values equal <span class="math inline">\(j\)</span>:</p>
<p><span class="math display">\[\Pr(Y = j|X = x_0) = \frac{1}{K} \sum_{i \in \mathcal N_0} I(y_i = j).\]</span></p>
<p>Finally, KNN applies Bayes rule and classifies the test observation <span class="math inline">\(x_0\)</span> to the class with the largest probability.</p>
<p>The choice of <span class="math inline">\(K\)</span> has a drastic effect on the KNN classifier obtained. As <span class="math inline">\(K\)</span> grows, the method becomes less flexible and produces a decision boundary that is close to linear. This corresponds to a low-variance but high-bias classifier. In general, as we use more flexible classification methods, the training error rate will decline but the test error rate may not. In both the regression and classification settings, choosing the correct level of flexibility is critical to the success of any statistical learning method. The bias-variance tradeoff, and the resulting U-shape in the test error, can make this a difficult task.</p>
<p>Nearest Neighbors can be good when the number of variables, <span class="math inline">\(p\)</span>, is small, i.e. <span class="math inline">\(p \leq 4\)</span> and for a large number of sample points. It is one of many techniques called smoothers, like kernel and spline smoothing. Unfortunately this method is very poor when <span class="math inline">\(p\)</span> is large, since possible nearby neighbors tend to be far away in high dimensions. This is known as the <em>curse of dimensionality</em>. We need to get a reasonable fraction of the <span class="math inline">\(N\)</span> values of <span class="math inline">\(y_i\)</span> to average in order to bring the variance of our model down. However, as we increase the dimensions, the radius we need to search increases and we lose the efficacy of estimating using local averages.</p>
<h1 id="linear-regression">Linear Regression</h1>
<p><em>Linear regression</em> is a very simple approach for supervised learning and is a useful tool for predicting a quantitative response.</p>
<h2 id="simple-linear-regression">Simple Linear Regression</h2>
<p>Simple linear regression is a very straightforward approach for predicting a quantitative response <span class="math inline">\(Y\)</span> on the basis of a single predictor variable <span class="math inline">\(X\)</span>. It assumes that there is approximately a linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. We will sometimes describe this by saying that we are regressing <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span>. <span class="math display">\[Y \approx \beta_0 + \beta_1 X\]</span> <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are two unknown constants that represent the intercept and slope terms in the linear model and are known as its coefficients or parameters. We may use our training data to produce estimates <span class="math inline">\(\hat \beta_0\)</span> and <span class="math inline">\(\hat \beta_1\)</span> for prediction, <span class="math display">\[\hat y  \approx \hat \beta_0 + \hat \beta_1 x,\]</span> where <span class="math inline">\(\hat y\)</span> indicates a prediction of <span class="math inline">\(Y\)</span> on the basis of <span class="math inline">\(X = x\)</span>.</p>
<h3 id="estimating-the-coefficients">Estimating the Coefficients</h3>
<p>Let <span class="math inline">\((x_1, y_1), (x_2, y_2),..., (x_n, y_n)\)</span> represent <span class="math inline">\(n\)</span> observation pairs, each of which consists of a measurement of <span class="math inline">\(X\)</span> and a measurement of <span class="math inline">\(Y\)</span>. We want to find an intercept <span class="math inline">\(\hat \beta_0\)</span> and a slope <span class="math inline">\(\hat \beta_1\)</span> such that the resulting line is as close as possible to the <span class="math inline">\(n\)</span> data points. The most common definition of closeness involves minimizing the least squares criterion.</p>
<p>Let <span class="math inline">\(\hat y_i = \hat \beta_0 + \hat \beta_1 x_i\)</span> be the prediction for <span class="math inline">\(Y\)</span> based on the <span class="math inline">\(i\)</span>th value of <span class="math inline">\(X\)</span>. Then <span class="math inline">\(e_i = y_i - \hat y_i\)</span> represents the <span class="math inline">\(i\)</span>th residual – this is the difference between <span class="math inline">\(i\)</span>th observed response and its prediction.</p>
<p>We define the <em>residual sum of squares (RSS)</em> as, <span class="math display">\[\begin{aligned}
\label{eq:RSS}
\begin{split}
     RSS &amp;= e_1^2 + e_2^2 + \dots + e_n^2\\
     &amp;=  (y_1 - \hat \beta_0 - \hat \beta_1 x_1)^2 + \cdots + 
     (y_n - \hat \beta_0 - \hat \beta_1 x_n)^2
\end{split}\end{aligned}\]</span></p>
<p>The least squares approach chooses <span class="math inline">\(\hat \beta_0\)</span> and <span class="math inline">\(\hat \beta_1\)</span> to minimize the RSS.</p>
<p>Let <span class="math inline">\(\bar y \equiv \frac{1}{n} \sum^n_{i=1} y_i\)</span> and <span class="math inline">\(\bar x \equiv \frac{1}{n} \sum_{i=1}^n x_i\)</span> be the sample means. Then the the <em>least squares coefficient estimates</em> are given by,</p>
<p><span class="math display">\[\begin{aligned}
    \hat \beta_0 &amp;= \frac{\sum_{i=1}^n (x_i - \bar x) (y_i - \bar y)}{\sum_{i=1}^n (x_i - \bar x)^2}, \\ 
    \hat \beta_1 &amp;= \bar y - \hat \beta_1 \bar x.\end{aligned}\]</span></p>
<h3 id="assessing-the-accuracy-of-the-coefficient-estimates">Assessing the Accuracy of the Coefficient Estimates</h3>
<p>If <span class="math inline">\(f\)</span> is to be approximated by a linear function, then we can write this relationship as a <em>population regression line</em>, <span class="math display">\[\begin{aligned}
 Y &amp;= f(X) +  \epsilon \\
 Y  &amp;= \beta_0 +  \beta_1 X + \epsilon\end{aligned}\]</span> Where <span class="math inline">\(\beta_0\)</span> is the intercept, i.e. the expected value of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X = 0\)</span>, <span class="math inline">\(\beta_1\)</span> is the slope, i.e. the average increase in <span class="math inline">\(Y\)</span> in a unit of increase of <span class="math inline">\(X\)</span>, and <span class="math inline">\(\epsilon\)</span> is a catch-all error term.</p>
<p>The <em>least squares line</em> can always be computed using the coefficient estimates, however, the population regression line is unobserved. Fundamentally, the concept of these two lines is a natural extension of the standard statistical approach of using information from a sample to estimate characteristics of an unmeasured large population.</p>
<p>The analogy between linear regression and estimation of the mean of a random variable is an apt one based on the concept of bias. if we could average a huge number of estimates <span class="math inline">\(\hat \mu\)</span> of the true population mean <span class="math inline">\(\mu\)</span> obtained from a huge number of sets of observations, then this average would exactly equal <span class="math inline">\(\mu\)</span>. Thus it is an unbiased estimator, i.e. it does not systematically over- or under-estimate the true parameter.</p>
<p>To determine the accuracy of a sample mean <span class="math inline">\(\hat \mu\)</span> as an estimate of <span class="math inline">\(\mu\)</span>, we can find the <em>standard error</em>, which roughly tells us the average amount that this estimate differs from the actual value of <span class="math inline">\(\mu\)</span>, <span class="math display">\[\text{Var} (\hat \mu) = \text{SE} (\hat \mu ) ^2 = \frac{\sigma^2}{n},\]</span> where <span class="math inline">\(\sigma\)</span> is the standard deviation of each of the realizations <span class="math inline">\(y_i\)</span> of <span class="math inline">\(Y\)</span>.</p>
<p>To compute the standard errors associated with <span class="math inline">\(\hat \beta_0\)</span> and <span class="math inline">\(\hat \beta_1\)</span>,</p>
<p><span class="math display">\[\begin{aligned}
    \text{SE} ( \hat \beta_0 )^2 &amp;= \sigma^2 \Bigg (  \frac{\bar x^2}{(x_i - \bar x)^2}  \Bigg )\\ 
    \text{SE} ( \hat \beta_1 )^2 &amp;= \frac{\sigma^2}{\sum_{i=1}^n (x_i - \bar x)^2} \end{aligned}\]</span></p>
<p>where <span class="math inline">\(\sigma^2 = \text{Var}(\epsilon)\)</span>. In general, <span class="math inline">\(\sigma^2\)</span> is not known, but can be estimated from the data. The estimate of <span class="math inline">\(\sigma\)</span> is known as the <em>residual standard error</em>, and is given by the formula, <span class="math display">\[\label{eq:RSE-1}
    RSE = \sqrt{RSS/(n - 2)}\]</span></p>
<p>Standard errors can be used to compute <em>confidence intervals</em>. For example, a 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter.For linear regression the confidence interval of the coefficients is given by, <span class="math display">\[\begin{aligned}
    \hat \beta_1 &amp;\pm 2 \cdot \text{SE}(\hat \beta_1)  \\
    \hat \beta_0 &amp;\pm 2 \cdot \text{SE}(\hat \beta_0).\end{aligned}\]</span> Standard errors can also be used to perform hypothesis tests on the coefficients. The null hypotheses – <span class="math inline">\(H_0\)</span> : There is no relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, i.e. <span class="math inline">\(\beta_1 = 0\)</span>. The alternative hypothesis – <span class="math inline">\(H_a\)</span> : There is some relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, i.e. <span class="math inline">\(\beta_1 \neq 0\)</span>.</p>
<p>Notice that setting <span class="math inline">\(\beta_1 = 0\)</span> removes the <span class="math inline">\(X\)</span> term in <span class="math inline">\(Y = \beta_0 + \beta_1 X + \epsilon\)</span> and <span class="math inline">\(X\)</span> is not associated with <span class="math inline">\(Y\)</span>. So if <span class="math inline">\(\hat \beta_1\)</span> is sufficiently far from zero and <span class="math inline">\(\text{SE}(\hat \beta_1)\)</span> is small, then our estimates of the parameter our accurate and give strong evidence that the null hypotheses is true. if <span class="math inline">\(SE(\hat\beta_1)\)</span> is large, then <span class="math inline">\(SE(\hat\beta_1)\)</span> must be large in absolute value in order for us to reject the null hypothesis.</p>
<p>In practice, we compute the <em>t-statistic</em>, given by, <span class="math display">\[t = \frac{\hat \beta_1 - 0}{\text{SE}(\hat \beta_1)}\]</span> which measures the number of standard deviations that <span class="math inline">\(\hat \beta_1\)</span> is away from 0. If there is no relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the t-statistics will have a distribution with <span class="math inline">\(n - 2\)</span> degrees of freedom.</p>
<p>The <em>p-value</em> corresponds to the probability of observing any number equal to <span class="math inline">\(|t|\)</span> or larger in absolute value, assuming <span class="math inline">\(\beta_1= 0\)</span>. A small p-value indicates that it is unlikely to observe such a substantial association between the predictor and the response due to chance, in the absence of any real association, so we can infer that there is an association between the predictor and the response. Then, we can reject the null hypothesis and may declare a relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> if the p-value is small enough.</p>
<p>Simply put, a p-value is the probability that random chance generated the data, or (plus) it was the result of another event that is of equal or rarer probability. The traditional threshold for determining significance of a p-value is <span class="math inline">\(&lt; 0.05\)</span>.</p>
<h3 id="assessing-the-accuracy-of-the-model">Assessing the Accuracy of the Model</h3>
<p>Given the alternative hypothesis holds, we likely want to quantify the extent to which the model fits the data. A linear regression fit is typically assessed using two related quantities: the residual standard error (RSE) and the <span class="math inline">\(R^2\)</span> statistic.</p>
<h3 class="unnumbered" id="residual-error">Residual Error</h3>
<p>The RSE is an estimate of the standard deviation of the irreducible error term <span class="math inline">\(\epsilon\)</span>. Roughly speaking, it is the average amount that the response will deviate from the true regression line and is considered a measure of the lack of fit of the model to the data.</p>
<p>Recall, RSS is the residual sum of squares <a href="#eq:RSS" data-reference-type="eqref" data-reference="eq:RSS">[eq:RSS]</a> and the RSE was previously used to estimate <span class="math inline">\(\sigma\)</span> <a href="#eq:RSE-1" data-reference-type="eqref" data-reference="eq:RSE-1">[eq:RSE-1]</a>. Then, the RSE can be written as, <span class="math display">\[RSE = \sqrt{\frac{1}{(n-2)} \text{RSS} } 
    = \sqrt{\frac{1}{(n-2)} \sum_{i=1}^n (y_i - \hat y_i)^2 }.\]</span></p>
<h3 class="unnumbered" id="r2-statistic"><span class="math inline">\(R^2\)</span> Statistic</h3>
<p>The <span class="math inline">\(R^2\)</span> statistic provides an alternative measure of fit in terms of the proportion of explained variance, i.e. not residual, and so it always takes on a value between 0 and 1. Moreover, it is independent of the scale of <span class="math inline">\(Y\)</span>, unlike the RSE. The <span class="math inline">\(R^2\)</span> statistic is a measure of the linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>Let <span class="math inline">\(\text{TSS} = (y_i - \hat y)^2\)</span> be the <em>total sum of squares</em> and RSS be the residual sum of squares <a href="#eq:RSS" data-reference-type="eqref" data-reference="eq:RSS">[eq:RSS]</a>. Then <span class="math inline">\(R^2\)</span> is given by, <span class="math display">\[R^2 =  \frac{\text{TSS} - \text{RSS}}{\text{TSS}} = 1 - \frac{RSS}{TSS}\]</span></p>
<p>TSS can be thought of as the amount of variability inherent in the response before the regression is performed while RSS measures the amount of variability that is left unexplained after performing the regression. TSS - RSS measures the amount of variability in the response that is explained (or removed) by performing the regression, and <span class="math inline">\(R^2\)</span> measures the proportion of variability in <span class="math inline">\(Y\)</span> that can be explained using <span class="math inline">\(X\)</span>.</p>
<p>An <span class="math inline">\(R^2\)</span> statistic that is close to 1 indicates that a large proportion of the variability in the response has been explained by the regression, while the converse holds for a value close to 0. The <span class="math inline">\(R^2\)</span> statistic is similar to correlation, but holds for a between a larger number of variables.</p>
<h2 id="multiple-linear-regression">Multiple Linear Regression</h2>
<p>We can handle multiple predictors by giving each predictor a separate slope coefficient in a single model. In general, suppose that we have <span class="math inline">\(p\)</span> distinct predictors. Then the multiple linear regression model takes the form, <span class="math display">\[Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p + \epsilon,\]</span> where <span class="math inline">\(X_j\)</span> represents the <span class="math inline">\(j\)</span>th predictor and <span class="math inline">\(\beta_j\)</span> quantifies the association between that variable and the response, namely the average effect on <span class="math inline">\(Y\)</span> of a one unit increase in <span class="math inline">\(X_j\)</span> while holding all other predictors fixed.</p>
<h3 id="estimating-the-regression-coefficients">Estimating the Regression Coefficients</h3>
<p>Given estimates <span class="math inline">\(\hat \beta_0, \hat \beta_1,..., \hat \beta_p\)</span>, we can make predictions using the formula, <span class="math display">\[\hat y= \hat \beta_0, \hat \beta_1 x_1,..., \hat \beta_p x_p.\]</span> Again, we choose <span class="math inline">\(\beta_0, \beta_1,...,\beta_p\)</span> to minimize the residual sum of squared, <span class="math display">\[\begin{aligned}
 \label{eq:RSS-sum}
    RSS &amp;= \sum_{i=1}^{n} (y_i - \hat y_i)^2 \\
    &amp;=  \sum_{i=1}^{n}(y_i - \hat \beta_0 - \hat \beta_1 x_1,..., \hat \beta_p x_p)^2 \\
    &amp;= \sum_{i=1}^n \Bigg ( y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \Bigg)^2\end{aligned}\]</span> The values <span class="math inline">\(\hat \beta_0, \hat \beta_1 x_1,..., \hat \beta_p x_p\)</span> that minimize the above equation are the multiple least squares regression coefficient estimates. These values are most easily represented using matrix algebra and will be revisited later.</p>
<h3 id="some-important-questions">Some Important Questions</h3>
<ol type="1">
<li><p>Is at least one of the predictors <span class="math inline">\(X_1, X_2,...,X_p\)</span> useful in predicting the response?</p></li>
<li><p>Do all the predictors help to explain <span class="math inline">\(Y\)</span>, or is only a subset of the predictors useful?</p></li>
<li><p>How well does the model fit the data?</p></li>
<li><p>Given a set of predictor values, what response value should we predict, and how accurate is our prediction?</p></li>
</ol>
<h3 class="unnumbered" id="is-there-a-relationship-between-the-response-and-predictors">Is There a Relationship Between the Response and Predictors?</h3>
<p>In the multiple regression setting with <span class="math inline">\(p\)</span> predictors, we need to ask whether all of the regression coefficients are zero. The null hypothesis becomes, <span class="math display">\[H_0 : \beta_1 = \beta_2 = \dots = \beta_p = 0.\]</span></p>
<p>This hypothesis test is performed by computing the <em>F-statistic</em>. <span class="math display">\[F = \frac{(\text{TSS} - \text{RSS})/p }{\text{RSS}/ (n-p-1)}\]</span></p>
<p>When there is no relationship between the response and predictors, one would expect the F-statistic to take on a value close to 1. On the other hand, if <span class="math inline">\(H_a\)</span> is true (i.e. at least one <span class="math inline">\(\beta_j\)</span> is non-zero.), then <span class="math inline">\(E{(\text{TSS} - \text{RSS})/p} &gt; \sigma^2\)</span>, so we expect <span class="math inline">\(F\)</span> to be greater than <span class="math inline">\(1\)</span>.</p>
<p>The approach of using an F-statistic to test for any association between the predictors and the response works when <span class="math inline">\(p\)</span> is relatively small, and certainly small compared to <span class="math inline">\(n\)</span>. However, sometimes we have a very large number of variables. If <span class="math inline">\(p&gt;n\)</span> then there are more coefficients <span class="math inline">\(\beta_j\)</span> to estimate than observations from which to estimate them. In this case we cannot even fit the multiple linear regression model using least squares, so the F-statistic cannot be used. Approaches like <em>forward selection</em> can be used instead.</p>
<h3 id="deciding-on-important-variables">Deciding on Important Variables</h3>
<p>The first step in a multiple regression analysis is to compute the F-statistic and to examine the associated p-value. If we conclude on the basis of the p-value that at least one of the predictors is related to the response, then it is natural to wonder which are the relevant ones. The task of determining which predictors are associated with the response, in order to fit a single model involving only those predictors, is referred to as <em>variable selection</em>, discussed more in detail in chapter 6.</p>
<p>There are a total of <span class="math inline">\(2^p\)</span> models that contain subsets of <span class="math inline">\(p\)</span> variables. In general, we cannot consider all <span class="math inline">\(2^p\)</span> models, and instead we need an automated and efficient approach to choose a smaller set of models to consider. There are three classical approaches for this task:</p>
<ol type="1">
<li><p><em>Forward selection</em> – We begin with the null model, i.e. a model that contains an intercept (i.e <span class="math inline">\(\beta_0\)</span>) but no predictors. We then fit <span class="math inline">\(p\)</span> simple linear regressions and add to the null model the variable that results in the lowest RSS. We then add to that model the variable that results in the lowest RSS for the new two-variable model. This approach is continued until some stopping rule is satisfied.</p></li>
<li><p><em>Backward selection</em> – We start with all variables in the model, and remove the variable with the largest p-value—that is, the variable that is the least statistically significant. The new (<span class="math inline">\(p - 1\)</span>)-variable model is fit, and the variable with the largest p-value is removed. This procedure continues until a stopping rule is reached, i.e. when all remaining variables have a p-value below some threshold.</p></li>
<li><p><em>Mixed selection</em> – This is a combination of forward and backward selection. We start with no variables in the model, and as with forward selection, we add the variable that provides the best fit. Since the p-values for variables can become larger as new predictors are added to the model, if at any point the p-value for one of the variables in the model rises above a certain threshold, then we remove that variable from the model. We continue to perform these forward and backward steps until all variables in the model have a sufficiently low p-value.</p></li>
</ol>
<p>Backward selection cannot be used if <span class="math inline">\(p&gt;n\)</span>, while forward selection can always be used. Forward selection is a greedy approach, and might include variables early that later become redundant. Mixed selection can remedy this.</p>
<h3 class="unnumbered" id="model-fit">Model Fit</h3>
<p>In multiple linear regression, <span class="math inline">\(R^2 = Cor(Y, \hat Y)^2\)</span>, the square of the correlation between the response and the fitted linear model. <span class="math inline">\(R^2\)</span> will always increase when more variables are added to the model, even if they are only weakly associated with the response. This is due to the fact that adding another variable to the least squares equations must allow us to fit the training data more accurately, so the <span class="math inline">\(R^2\)</span> statistic, which is also computed on the training data, must increase.</p>
<h3 class="unnumbered" id="predictions">Predictions</h3>
<p>After fitting a regression model, there are three sorts of uncertainty associated with a prediction.</p>
<ol type="1">
<li><p>There will be some inaccuracy in the coefficient estimates related to the reducible error of using the least squares plane. We can compute a <em>confidence interval</em> in order to determine how close <span class="math inline">\(\hat Y\)</span> will be to <span class="math inline">\(f(X)\)</span>.</p></li>
<li><p>There is potentially an additional source of reducible error, called <em>model bias</em>, from using a linear model to represent complex non-linear data.</p></li>
<li><p>There will be some amount of irreducible or random error in the model. <em>Prediction intervals</em> are used to incorporate both the error in the estimate for <span class="math inline">\(f(X)\)</span> (the reducible error) as is done in a confidence interval, as well as the uncertainty as to how much an individual point will differ from the population regression plane (the irreducible error).</p></li>
</ol>
<h2 id="other-considerations-in-the-regression-model">Other Considerations in the Regression Model</h2>
<h3 id="qualitative-predictors">Qualitative Predictors</h3>
<h3 class="unnumbered" id="predictors-with-only-two-levels">Predictors with Only Two Levels</h3>
<p>If a qualitative predictor (also known as a factor) only has two levels, or possible values, then incorporating it into a regression model is very simple. We simply create an indicator or dummy variable that takes on two possible numerical values, i.e. (0 or 1) and use this variable as a predictor in the regression equation.</p>
<h3 class="unnumbered" id="qualitative-predictors-with-more-than-two-levels">Qualitative Predictors with More than Two Levels</h3>
<p>In this situation, we can create additional dummy variables. There will always be one fewer dummy variable than the number of levels. The level with no dummy variable is known as the <em>baseline</em>.</p>
<h3 id="extensions-of-the-linear-model">Extensions of the Linear Model</h3>
<p>Two of the most impactful assumptions in the standard linear regression model state that the relationship between the predictors and response are additive and linear. The <em>additive</em> assumption means that the effect of changes in a predictor <span class="math inline">\(X_j\)</span> on the response Y is independent of the values of the other predictors. The <em>linear</em> assumption states that the change in the response <span class="math inline">\(Y\)</span> due to a one-unit change in <span class="math inline">\(X_j\)</span> is constant, regardless of the value of <span class="math inline">\(X_j\)</span>.</p>
<h3 class="unnumbered" id="removing-the-additive-assumption">Removing the Additive Assumption</h3>
<p>One way of extending this model to allow for interaction effects is to include a third predictor, called an <em>interaction term</em> <span class="math inline">\(\Tilde \beta_1\)</span>, which is constructed by computing the product of X1 and X2. <span class="math display">\[\begin{aligned}
    Y &amp;= \beta_0 + \beta_1 X_ 1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \epsilon \\
    &amp;= \beta_0 + (\beta_1 + \beta_3 X_2)X_1 + \beta_2 X_2 + \epsilon \\
    &amp;= \beta_0 + \Tilde \beta_1 X_1 + \beta_2 X_2 + \epsilon\end{aligned}\]</span> Then, adjusting <span class="math inline">\(X_2\)</span> will change the impact of <span class="math inline">\(X_1\)</span> on <span class="math inline">\(Y\)</span>.</p>
<p>The <em>hierarchical principle</em> states that if we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant.</p>
<h3 class="unnumbered" id="non-linear-relationships">Non-linear Relationships</h3>
<p>In some cases, the true relationship between the response and the predictors may be nonlinear. A simple way to directly extend the linear model to accommodate non-linear relationships is to use <em>polynomial regression</em>, in which we include polynomial functions of the predictors in the regression model.</p>
<h3 id="potential-problems-and-troubleshooting">Potential Problems and Troubleshooting</h3>
<ol type="1">
<li><p>Non-linearity of the response-predictor relationships.</p>
<p>Residual plots are useful graphical tool for identifying non-linearity. If there is a strong pattern in the plot of the residuals, <span class="math inline">\(e_i = y_i - \hat y_i\)</span>, versus the predictor <span class="math inline">\(x_i\)</span>, there may be a problem with some aspect of the linear model.</p></li>
<li><p>Correlation of error terms.</p>
<p>In general, the assumption of uncorrelated errors is extremely important for linear regression as well as for other statistical methods.</p>
<p>If there is correlation among the error terms, then the estimated standard errors will tend to underestimate the true standard errors, confidence and prediction intervals will be narrower than they should be, and p-values associated with the model will be lower than they should be. Such correlations frequently occur in the context of time series data.</p>
<p>We can plot the residuals from our model as a function of time and if the errors are uncorrelated, then there should be no discernible pattern. Otherwise we may find that <em>tracking</em> exists, i.e. adjacent residuals often have similar values.</p></li>
<li><p>Non-constant variance of error terms.</p>
<p>An important assumption is that error terms have constant variance, i.e. <span class="math inline">\(\text{Var}(\epsilon_i) = \sigma^2\)</span>. One can identify non-constant variance, a.k.a. <em>heteroscedasticity</em>, from the presence of a funnel shape in the residual plot.</p>
<p>A simple remedy may be to fit our model by <em>weighted least squares</em>, with weights proportional to the inverse weighted variances.</p></li>
<li><p>Outliers.</p>
<p>A point for which <span class="math inline">\(y_i\)</span> is far from the value predicted by the model can occur for many reasons, possibly as a result of incorrect recording of an observation. It is typical for an outlier that does not have an unusual predictor value to have little effect on the least squares fit, but may have an effect on the RSE, used to compute all confidence intervals or p-values, as well as <span class="math inline">\(R^2\)</span>.</p>
<p>Residual plots can be used to identify outliers, but in practice, it can be difficult to decide how large a residual needs to be before we consider the point to be an outlier. Instead, we can plot the <em>studentized residuals</em>, computed by dividing each residual <span class="math inline">\(e_i\)</span> by its estimated standard error. Observations whose studentized residuals are greater than 3 in absolute value are possible outliers and can be removed or examined to determine a deficiency with the model, such as a missing predictor.</p></li>
<li><p>High-leverage points.</p>
<p>In contrast to outliers, observations with <em>high leverage</em> have an unusual value for <span class="math inline">\(x_i\)</span>. Removing a high leverage observation has a much more substantial impact on the least squares line than removing the outlier.</p>
<p>To quantify an observation’s leverage, we compute the <em>leverage statistic</em>. The leverage statistic <span class="math inline">\(h_i\)</span> is always between <span class="math inline">\(1/n\)</span> and <span class="math inline">\(1\)</span>, and the average leverage for all the observations is always equal to <span class="math inline">\((p + 1)/n\)</span>. For a simple linear regression, <span class="math display">\[\label{eq:leverage}
        h_i =  \frac{1}{n} \frac{(x_i - \bar x)^2}{\sum_{i&#39; = 1}^n (x_i - \bar x)^2}\]</span></p></li>
<li><p>Collinearity.</p>
<p><em>Collinearity</em> refers to the situation in which two or more predictor variables are closely related to one another. In the regression context, it can be difficult to separate out the individual effects of collinear variables on the response and reduces the accuracy of the estimates of the regression coefficients. This causes the standard error for <span class="math inline">\(\hat \beta_j\)</span> to grow and the t-statistic to decline which in turn means the power of the hypothesis test - the probability of correctly detecting a non-zero coefficient - is reduced by collinearity.</p>
<p>A simple way to detect collinearity is to look at the correlation matrix of the predictors, though this may not always work. In particular, it is possible for collinearity to exist between three or more variables in what’s called <em>multicollinearity</em>. In this situation it’s better to compute the <em>variance inflation factor</em> (VIF), the ratio of the variance of <span class="math inline">\(\hat \beta_j\)</span> when fitting the full model divided by the variance of <span class="math inline">\(\hat \beta_j\)</span> if fit on its own. A VIF value of 1 indicates the complete absence of collinearity, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity.</p>
<p><span class="math display">\[VIF(\hat \beta_j ) = \frac{1}{1-R^2_{X_j|X_{-j}}}\]</span> where <span class="math inline">\(R^2_{X_j|X_{-j}}\)</span> is the <span class="math inline">\(R^2\)</span> from a regression of <span class="math inline">\(X_j\)</span> onto all of the other predictors.</p>
<p>To mitigate collinerarity, we may drop one of the problematic variables from the regression, which will also reduce redundancy. Alternatively, we may combine the collinear variables together into a single predictor.</p>
<h2 id="comparison-of-linear-regression-with-k-nearest-neighbors">Comparison of Linear Regression with K-Nearest Neighbors</h2>
<p>Linear regression is an example of a parametric approach and has many advantages: they are often easy to fit, the coefficients have simple interpretations, and tests of statistical significance can be easily performed. But parametric methods do have a disadvantage: by construction, they make strong assumptions about the form of <span class="math inline">\(f(X)\)</span>.</p>
<p>In contrast, non-parametric methods do not explicitly assume a parametric form for <span class="math inline">\(f(X)\)</span>, and thereby provide an alternative and more flexible approach for performing regression.</p>
<p>One of the simplest and best-known non-parametric methods is <em>K-nearest neighbors regression</em> (KNN regression), which is closely related to the KNN classifier.</p>
<p>Given a value for <span class="math inline">\(K\)</span> and a prediction point <span class="math inline">\(x_0\)</span>, KNN regression first identifies the <span class="math inline">\(K\)</span> training observations that are closest to <span class="math inline">\(x_0\)</span>, represented by <span class="math inline">\(\mathcal N_0\)</span>. It then estimates <span class="math inline">\(f(x_0)\)</span> using the average of all the training responses in <span class="math inline">\(\mathcal N_0\)</span>. In general, the optimal value for K will depend on the bias-variance tradeoff.</p>
<p><span class="math display">\[\hat f (x_0) = \frac{1}{K} \sum_{x_i \in \mathcal N_0} y_i.\]</span></p>
<p>The parametric approach will outperform the nonparametric approach if the parametric form that has been selected is close to the true form of <span class="math inline">\(f\)</span>. Even when the true relationship is highly non-linear, KNN may still provide inferior results to linear regression. A decrease in performance as the dimension increases is a common problem for KNN, and results from the fact that in higher dimensions there is effectively a reduction in sample size. As a general rule, parametric methods will tend to outperform non-parametric approaches when there is a small number of observations per predictor.</p>
<h1 id="classification">Classification</h1>
<h2 id="an-overview-of-classification">An Overview of Classification</h2>
<p>The linear regression model assumes that the response variable <span class="math inline">\(Y\)</span> is quantitative. But in many situations, the response variable is instead qualitative. Though just as in the regression setting, in the classification setting we have a set of training observations <span class="math inline">\((x_1, y_1),\dots,(x_n, y_n)\)</span>.</p>
<h2 id="why-not-linear-regression">Why Not Linear Regression</h2>
<p>Linear regression is not appropriate in the case of a qualitative response since codings of qualtative states into numeric values may produce unwarranted implicit relations with each of the codings depending on their orderings. This can produce fundamentally different linear models that would ultimately lead to different sets of predictions on test observations.</p>
<p>For a binary (two level) qualitative response we could potentially use the dummy variable approach to code the response and could then fit a linear regression to it. However, if we use linear regression, some of our estimates might beoutside the [0, 1] interval, making them hard to interpret as probabilities. Nevertheless, the predictions provide an ordering and can be interpreted as crude probability estimates. It turns out that the classifications that we get if we use linear regression to predict a binary response will be the same as for the linear discriminant analysis (LDA) procedure discussed later.</p>
<h2 id="logistic-regression">Logistic Regression</h2>
<h3 id="the-logistic-model">The Logistic Model</h3>
<p>Any time a straight line is fit to a binary response that is coded as <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>, in principle we can always predict <span class="math inline">\(p(X) &lt; 0\)</span> for some values of <span class="math inline">\(X\)</span> and <span class="math inline">\(p(X) &gt; 1\)</span> for others (unless the range of <span class="math inline">\(X\)</span> is limited). Instead, we must model <span class="math inline">\(p(X)\)</span> using a function that gives outputs between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> for all values of <span class="math inline">\(X\)</span>. Many functions meet this description. In logistic regression, we use the logistic function, <span class="math display">\[p(X) = \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}\]</span> To fit the model, we use a method called <em>maximum likelihood</em>, discussed in the next section. The logistic function will always produce an S-shaped curve, and so regardless of the value of <span class="math inline">\(X\)</span>, we will obtain a sensible prediction. After a bit of manipulation, we find that, <span class="math display">\[\frac{p(X)}{1-p(X)} = e^{\beta_0+\beta_1 X}\]</span> The quantity on the left-hand side is called the <em>odds</em> and can take on any value between 0 and <span class="math inline">\(\infty\)</span>, indicating very low and high probabilities. Odds are traditionally used instead of probabilities in horse-racing, since they relate more naturally to the correct betting strategy.</p>
<p>By taking the logarithm of both sides, <span class="math display">\[\log \Bigg ( \frac{p(X)}{1-p(X)} \Bigg ) = \beta_0+\beta_1 X\]</span></p>
<p>The left-hand side is called the <em>log-odds</em> or <em>logit</em>. We see that the logistic regression model has a logit that is linear in <span class="math inline">\(X\)</span>.</p>
<p>Recall that in a linear regression model, <span class="math inline">\(\beta_1\)</span> gives the average change in <span class="math inline">\(Y\)</span> associated with a one-unit increase in <span class="math inline">\(X\)</span>. In contrast, in a logistic regression model, increasing <span class="math inline">\(X\)</span> by one unit changes the log odds by <span class="math inline">\(\beta_1\)</span>, or equivalently it multiplies the odds by <span class="math inline">\(e^{\beta_1}\)</span>. The amount that <span class="math inline">\(p(X)\)</span> changes due to a one-unit change in <span class="math inline">\(X\)</span> will depend on the current value of <span class="math inline">\(X\)</span>, though if <span class="math inline">\(\beta_1\)</span> is positive then increasing <span class="math inline">\(X\)</span> will be associated with increasing <span class="math inline">\(p(X)\)</span>, and if <span class="math inline">\(\beta_1\)</span> is negative then increasing <span class="math inline">\(X\)</span> will be associated with decreasing <span class="math inline">\(p(X)\)</span>.</p>
<h3 id="estimating-the-regression-coefficients-1">Estimating the Regression Coefficients</h3>
<p>The coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are unknown, and must be estimated based on the available training data. <em>Maximum likelihood</em> is used to fit a logistic regression model instead of least squares. The estimates <span class="math inline">\(\hat \beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are chosen to maximize what’s called the <em>likelihood function</em>: <span class="math display">\[l(\beta_0,\beta_1) = \prod_{i:y_i = 1} p(x_i) \prod_{i&#39;:y_{i&#39; = 1}} (1-p(x_{i&#39;}))\]</span> Maximum likelihood is a very general approach that is used to fit many non-linear models. In the linear regression setting, the least squares approach is in fact a special case of maximum likelihood.</p>
<p>Many aspects of the logistic regression are similar to the linear regression output. For example, we can measure the accuracy of the coefficient estimates by computing their standard errors. The <em><span class="math inline">\(z\)</span>-statistic</em> plays the same role as the <span class="math inline">\(t\)</span>-statistic in the linear regression output. the <span class="math inline">\(z\)</span>-statistic associated with <span class="math inline">\(\beta_1\)</span> is equal to <span class="math inline">\(\hat \beta_1/SE(\hat\beta_1)\)</span>, and so a large (absolute) value of the <span class="math inline">\(z\)</span>-statistic indicates evidence against the null hypothesis <span class="math inline">\(H_0 : \beta_1 = 0\)</span>. This null hypothesis implies that <span class="math inline">\(p(X) = \frac{e^{\beta_0}}{1+e^{\beta_0}}\)</span>.</p>
<h3 id="making-predictions">Making Predictions</h3>
<p>Once the coefficients have been estimated, it is a simple matter to compute the probability. We can then use qualitative predictors with the logistic regression model to fit the model by using the dummy variable approach mentioned previously, i.e. the variable takes on a value of 1 or 0. If the coefficient associated with the dummy variable is positive and the associated p-value is statistically significant, this will validate that there is a higher probability associated with the encoded qualitative state.</p>
<h3 id="multiple-logistic-regression">Multiple Logistic Regression</h3>
<p>By analogy with the extension from simple to multiple linear regression, we can generalize the logistic model as follows, <span class="math display">\[\log \Bigg ( \frac{p(X)}{1-p(X)} \Bigg ) = \beta_0+\beta_1 X_1 + \dots + \beta_pX_p,\]</span> where <span class="math inline">\(X = (X_1,...,X_p)\)</span> are <span class="math inline">\(p\)</span> predictors. This can be rewritten as, <span class="math display">\[p(X) = \frac{e^{\beta_0+\beta_1 X_1 + \dots + \beta_pX_p}}{1+e^{\beta_0+\beta_1 X_1 + \dots + \beta_pX_p}}\]</span> Just as before, we use the maximum likelihood method to estimate <span class="math inline">\(\beta_0, \beta_1,\dots,\beta_p\)</span>.</p>
<h3 id="logistic-regression-for-2-response-classes">Logistic Regression for <span class="math inline">\(&gt;2\)</span> Response Classes</h3>
<p>Suppose we want to classify a response variable that has more than two classes. The two-class logistic regression models discussed in the previous sections have multiple-class extensions, but in practice they tend not to be used all that often. Instead, a more popular multiple-class classification method known as, <em>discriminant analysis</em>, is used.</p>
<h2 id="linear-discriminant-analysis">Linear Discriminant Analysis</h2>
<p>Logistic regression involves directly modeling <span class="math inline">\(Pr(Y = k|X = x)\)</span> using the logistic function to model the conditional distribution of the response <span class="math inline">\(Y\)</span>, given the predictor(s) <span class="math inline">\(X\)</span>. In an alternative and less direct approach to estimating these probabilities, we model the distribution of the predictors <span class="math inline">\(X\)</span> separately in each of the response classes (i.e. given <span class="math inline">\(Y\)</span>), and then use Bayes’ theorem <a href="#eq:bayes" data-reference-type="eqref" data-reference="eq:bayes">[eq:bayes]</a> to flip these around into estimates for <span class="math inline">\(Pr(Y = k|X = x)\)</span>. When these distributions are assumed to be normal the model is very similar in form to logistic regression.</p>
<p>Linear discriminant analysis is a useful alternative to logistic regression because:</p>
<ul>
<li><p>When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.</p></li>
<li><p>If <span class="math inline">\(n\)</span> is small and the distribution of the predictors <span class="math inline">\(X\)</span> is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.</p></li>
<li><p>As mentioned, linear discriminant analysis is popular when we have more than two response classes.</p></li>
</ul>
<h3 id="using-bayes-theorem-for-classification">Using Bayes’ Theorem for Classification</h3>
<p>Suppose the qualitative response variable <span class="math inline">\(Y\)</span> can take on <span class="math inline">\(K&gt;2\)</span> possible distinct and unordered values. Let <span class="math inline">\(\pi_k\)</span> represent the overall or <em>prior</em> probability that a randomly chosen observation comes from the <span class="math inline">\(k\)</span>th class or category of the response variable <span class="math inline">\(Y\)</span>.</p>
<p>If <span class="math inline">\(X\)</span> is a discrete random variable, Let <span class="math inline">\(f_k(X) = \Pr(X = x|Y = k)\)</span> denote the <em>density function</em> of <span class="math inline">\(X\)</span> for an observation that comes from the <span class="math inline">\(k\)</span>th class. The densiity function is relatively large if there is a high probability that an observation in the kth class has <span class="math inline">\(X \approx x\)</span> and small otherwise. Then <em>Bayes’ theorem</em> states that,</p>
<p><span class="math display">\[\label{eq:bayes}
    \Pr(Y = k| X = x) = \frac{\pi_k f_k (x) }{ \sum_{l=1}^{K} \pi_l f_l(x)}.\]</span></p>
<p>Recall, <span class="math inline">\(p_k(X) = \Pr(Y = k|X)\)</span>. We refer to <span class="math inline">\(p_k(x)\)</span> as the <em>posterior</em> probability that an observation belongs to the kth class, given the predictor value for that observation, i.e. <span class="math inline">\(X = x\)</span>. This suggests that instead of directly computing <span class="math inline">\(p_k(X)\)</span> as was done in previous methods, we can plug in estimates of <span class="math inline">\(\pi_k\)</span> and <span class="math inline">\(f_k(X)\)</span> into the above formula. Estimating the pripor probabilities <span class="math inline">\(\pi_k\)</span> is easy if we have a random sample of <span class="math inline">\(Y\)</span>s from the population: we simply compute the fraction of the training observations that belong to the <span class="math inline">\(k\)</span>th class. However, estimating <span class="math inline">\(f_k(X)\)</span> tends to be more challenging, unless we assume some simple forms for these densities.</p>
<p>Recall the Bayes classifier, which classifies an observation to the class for which <span class="math inline">\(p_k(X)\)</span> is largest, has the lowest possible error rate out of all classifiers. Therefore, if we can find a way to estimate <span class="math inline">\(f_k(X)\)</span>, then we can develop a classifier that approximates the Bayes classifier.</p>
<h3 id="linear-discriminant-analysis-for-p1">Linear Discriminant Analysis for <span class="math inline">\(p=1\)</span></h3>
<p>Assume that we only have one predictor, i.e. <span class="math inline">\(p=1\)</span>. To obtain an estimate for <span class="math inline">\(f_k(x)\)</span> to plug into the Linear Discriminant Analysis formula we will first make some assumptions about its form.</p>
<p>Suppose we assume that <span class="math inline">\(f_k(x)\)</span> is <em>normal</em> or <em>Gaussian</em>. In the one-dimensional setting, the normal density takes the form, <span class="math display">\[\label{eq:gaussian}
    f_k(x) = \frac{1}{\sqrt{2\pi} \sigma_k} \exp \Bigg ( -\frac{1}{2\sigma^2_k} (x-\mu_k)^2 \Bigg)\]</span></p>
<p>where <span class="math inline">\(\mu_k\)</span> and <span class="math inline">\(\sigma^2_k\)</span> are the mean and variance parameters for the <span class="math inline">\(k\)</span>th class. We may assume there is a shared variance term across all <span class="math inline">\(K\)</span> classes, i.e. <span class="math inline">\(\sigma^2_1 = \dots = \sigma^2_K\)</span>, which for simplicity we can denote by <span class="math inline">\(\sigma^2\)</span>.</p>
<p>We can then plug this into Bayes’ theorem <a href="#eq:bayes" data-reference-type="eqref" data-reference="eq:bayes">[eq:bayes]</a>, <span class="math display">\[p_k(x) = \frac{
        \pi_k \frac{1}{\sqrt(2\pi) \sigma_k} \exp \Big ( -\frac{1}{2\sigma^2_k} (x-\mu_k)^2 \Big)  
    }{
         \sum_{l=1}^{K} \pi_l \frac{1}{\sqrt(2\pi) \sigma_k} \exp \Big ( -\frac{1}{2\sigma^2_k} (x-\mu_k)^2 \Big) 
    }.\]</span></p>
<p>Taking the log and rearranging the terms shows us that assigning an observation in a Bayes’ classifier is equivalent to assigning the observation to the class for which the following equation is the largest:</p>
<p><span class="math display">\[\label{eq:lda-delta}
    \delta_k(x) = x \cdot \frac{\mu_k}{\sigma^2} -\frac{\mu_k^2}{2\sigma^2} + \log(\pi_k)\]</span></p>
<p>In practice, even if we are quite certain of our assumption that <span class="math inline">\(X\)</span> is drawn from a Gaussian distribution within each class, we still have to estimate the parameters <span class="math inline">\(\mu_1,...,\mu_K\)</span>, <span class="math inline">\(\pi_1,..., \pi_K\)</span>, and <span class="math inline">\(\sigma^2\)</span>. The <em>linear discriminant analysis</em> (LDA) method approximates the Bayes classifier by plugging estimates for <span class="math inline">\(\pi_k, \mu_k\)</span>, and <span class="math inline">\(\sigma^2\)</span> into the above <a href="#eq:lda-delta" data-reference-type="eqref" data-reference="eq:lda-delta">[eq:lda-delta]</a>. In particular, the following estimates are used: <span class="math display">\[\begin{aligned}
\begin{split}
    \hat \mu &amp;=  \frac{1}{n^k} \sum_{i:y_i = k} x_i \\
    \hat \sigma^2 &amp;= \frac{1}{n-K} \sum_{k=1}^K \sum_{i:y_i = k} (x_i - \hat \mu_k)^2 
\end{split}\end{aligned}\]</span> where <span class="math inline">\(n\)</span> is the total number of training observations and <span class="math inline">\(n_k\)</span> is the number of training observations in the <span class="math inline">\(k\)</span>th class. The estimate for <span class="math inline">\(\mu_k\)</span> is simply the average of all the training observations from the <span class="math inline">\(k\)</span>th class, while <span class="math inline">\(\hat\sigma^2\)</span> can be seen as a weighted average of the sample variances for each of the <span class="math inline">\(K\)</span> classes.</p>
<p>Sometimes we have knowledge of the class membership probabilities <span class="math inline">\(\pi_1,...,\pi_K\)</span>, otherwise LDA estimates <span class="math inline">\(\pi_k\)</span> using the proportion of the training observations that belong to the <span class="math inline">\(k\)</span>th class, i.e, <span class="math display">\[\hat \pi_k = \frac{n_k}{n}.\]</span></p>
<p>When plugging the above estimates into <span class="math inline">\(\delta_k\)</span> <a href="#eq:lda-delta" data-reference-type="eqref" data-reference="eq:lda-delta">[eq:lda-delta]</a>, they become the <em>discriminant functions</em>, <span class="math inline">\(\hat \delta_k(x)\)</span>. The word linear in the classifier’s name stems from the fact that the discriminant functions <span class="math inline">\(\hat \delta_k\)</span> are linear functions of <span class="math inline">\(x\)</span>. Again, we assign an observation <span class="math inline">\(X = x\)</span> to the class for which <span class="math inline">\(\hat \delta_k\)</span> is largest: <span class="math display">\[\hat \delta_k(x) = x \cdot \frac{\hat \mu_k}{\hat \sigma^2} -\frac{\hat \mu_k^2}{2 \hat \sigma^2} + \log(\hat \pi_k).\]</span></p>
<p>To reiterate, the LDA classifier results from assuming that the observations within each class come from a normal distribution with a class-specific mean vector and a common variance <span class="math inline">\(\sigma^2\)</span>, then plugging estimates for these parameters into the Bayes classifier.</p>
<h3 id="linear-discriminant-analysis-for-p1-1">Linear Discriminant Analysis for <span class="math inline">\(p&gt;1\)</span></h3>
<p>To extend the LDA classifier to the case of multiple predictors we will assume that <span class="math inline">\(X = (X_1, X_2,...,X_p)\)</span> is drawn from a <em>multivariate Gaussian</em> (or <em>multivariate normal</em>) distribution, with a class-specific mean vector and a common covariance matrix.</p>
<p>The multivariate Gaussian distribution assumes that each individual predictor follows a one-dimensional normal distribution with some correlation between each pair of predictors. To indicate that a p-dimensional random variable <span class="math inline">\(X\)</span> has a multivariate Gaussian distribution, we write <span class="math inline">\(X \sim N(\mu, \Sigma)\)</span>. Here <span class="math inline">\(E(X) = \mu\)</span> is the mean of <span class="math inline">\(X\)</span> (a vector with <span class="math inline">\(p\)</span> components), and <span class="math inline">\(Cov(X) = \Sigma\)</span> is the <span class="math inline">\(p \times p\)</span> covariance matrix of <span class="math inline">\(X\)</span>.</p>
<p>The Gaussian density is defined as, <span class="math display">\[f(x) = \frac{1}{(2\pi)^{p/2} |\Sigma|^{1/2}} \exp \Bigg( -\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu) \Bigg)\]</span></p>
<p>Then, in the case of <span class="math inline">\(p &gt; 1\)</span> predictors, the LDA classifier assumes observations in the <span class="math inline">\(k\)</span>th class are drawn from a multivariate Gaussian distribution <span class="math inline">\(N(\mu_k, \Sigma)\)</span>, where <span class="math inline">\(\mu_k\)</span> is a class-specific mean vector, and <span class="math inline">\(\Sigma\)</span> is a covariance matrix that is common to all <span class="math inline">\(K\)</span> classes.</p>
<p>Using this density function we see the Bayes classifier now assigns an observation <span class="math inline">\(X = x\)</span> to the class for which <span class="math inline">\(\delta_k(x)\)</span> is the largest. This is essentially the vector/matrix version of the discriminant functions, <span class="math display">\[\delta_k(x) = x^T \Sigma^{-1}\mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1}\mu_k + \log \pi_k.\]</span></p>
<p>The formulas for estimating the unknown parameters <span class="math inline">\(\mu_1,\dots,\mu_K\)</span>, <span class="math inline">\(\pi_1,...,\pi_K\)</span>, and <span class="math inline">\(\Sigma\)</span> are similar to those used in the one-dimensional case. To assign a new observation X = x, LDA plugs these estimates into the multi-dimensional discriminant functions and classifies to the class for which <span class="math inline">\(\hat \delta_k(x)\)</span> is largest. Note, the LDA decision rule still only depends on <span class="math inline">\(x\)</span> only through a linear combination of its elements.</p>
<p>Class-specific performance is also important in medicine and biology, where the terms <em>sensitivity</em>, corresponding to true positive classifications, and <em>specificity</em>, corresponding to true negative classifications, characterize the performance of a model or screening test.</p>
<p>The Bayes classifier works by assigning an observation to the class for which the posterior probability <span class="math inline">\(p_k(X)\)</span> is greatest. Thus, the Bayes classifier, and by extension LDA, uses a threshold of 50% for the posterior probability in order to assign an observation to a class. However, if we are more concerned about incorrectly predicting the class status, then we can consider lowering this threshold. This may increase the overall error but may make for more accurate identification of a certain class.</p>
<figure>
<img src="err-threshold.png" style="width:10cm" alt="" /><figcaption>image</figcaption>
</figure>
<p>Using a threshold of 0.5 minimizes the overall error rate, shown as a black solid line. This is to be expected, since the Bayes classifier uses a threshold of 0.5 and is known to have the lowest overall error rate. As the threshold is reduced, the error rate among a specific classification decreases steadily (orange dotted line), but the error rate among the observations who do not belong to the class increases (blue dotted line).</p>
<p>Varying the classifier threshold changes its true positive and false positive rate. These are also called the sensitivity and one minus the specificity of our classifier. Below are tables of classifier names and measures.</p>
<figure>
<img src="classifier-names.png" style="width:10cm" alt="" /><figcaption>image</figcaption>
</figure>
<figure>
<img src="classifier-measures.png" style="width:10cm" alt="" /><figcaption>image</figcaption>
</figure>
<p>The denominators for the false positive and true positive rates are the actual population counts in each class. In contrast, the denominators for the positive predictive value and the negative predictive value are the total predicted counts for each class.</p>
<figure>
<img src="roc-curve.png" style="width:8cm" alt="" /><figcaption>image</figcaption>
</figure>
<p>The <em>ROC curve</em> (receiver operating characteristics) above is a popular graphic for simultaneously displaying the two types of errors for all possible thresholds. The overall performance of a classifier, summarized over all possible thresholds, is given by the <em>area under the curve</em> (AUC). An ideal ROC curve will hug the top left corner, so the larger the AUC the better the classifier.</p>
<figure>
<img src="confusion-matrix.png" style="width:8cm" alt="" /><figcaption>image</figcaption>
</figure>
<p>A <em>confusion matrix</em> is a way of visualizing predictions made by a classifier and is just a table showing the distribution of predictions for a specific class. The x-axis indicates the true class of each observation while the y-axis corresponds to the class predicted by the model.</p>
<h3 id="quadratic-discriminant-analysis">Quadratic Discriminant Analysis</h3>
<p>Similarly to LDA, the <em>quadratic discriminant analysis</em> (QDA) classifier results from assuming that the observations from each class are drawn from a Gaussian distribution, and from plugging estimates for the parameters into Bayes’ theorem in order to perform prediction. However, unlike LDA, QDA assumes that each class has its own covariance matrix, i.e. <span class="math inline">\(X \sim N(\mu_k, \Sigma_k)\)</span> where <span class="math inline">\(\Sigma_k\)</span> is a covariance matrix for the <span class="math inline">\(k\)</span>th class.</p>
<p>Now, the Bayes classifier assigns an observation <span class="math inline">\(X = x\)</span> to the class for which <span class="math inline">\(\delta_k\)</span> is largest. <span class="math display">\[\begin{aligned}
\begin{split}
    \delta_k &amp;= - \frac{1}{2}(x-\mu_k)^T \Sigma_k^{-1} (x-\mu_k) - \frac{1}{2}\log|\Sigma_k | + \log \pi_k \\
    &amp;= - \frac{1}{2} x^T \Sigma^{-1}_k x + x^T \Sigma^{-1}_k \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1}_k  \mu_k - \frac{1}{2} \log |\Sigma_k| + \log \pi_k
\end{split}\end{aligned}\]</span></p>
<p>The QDA classifier involves plugging estimates for <span class="math inline">\(\Sigma_k, \mu_k\)</span>, and <span class="math inline">\(\pi_k\)</span> into the above equation and then assigning an observation <span class="math inline">\(X = x\)</span> to the class for which this quantity is largest. Though unlike in the LDA, the quantity <span class="math inline">\(x\)</span> appears as a quadratic function.</p>
<p>The reason for using QDA over LDA involves the bias-variance tradeoff. When there are <span class="math inline">\(p\)</span> predictors, QDA must estimate <span class="math inline">\(K\)</span> covariance matrices which requires estimating <span class="math inline">\(K \frac{p(p+1)}{2}\)</span> quadratic parameters. Alternatively, LDA only estimates <span class="math inline">\(Kp\)</span> linear parameters.</p>
<p>This means that LDA is a much less flexible classifier than QDA, and so has substantially lower variance, i.e. it’s less prone to overfitting. This can potentially lead to improved prediction performance, but there is a trade-off: if LDA’s assumption that the <span class="math inline">\(K\)</span> classes share a common covariance matrix is badly off, then LDA can suffer from high bias. Roughly speaking, LDA tends to be a better bet than QDA if there are relatively few training observations and so reducing variance is crucial. In contrast, QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix for the <span class="math inline">\(K\)</span> classes is clearly untenable.</p>
<h2 id="a-comparison-of-classification-methods">A Comparison of Classification Methods</h2>
<p>So far we have covered 4 classification methods: K-nearest neighbors (KNN) logistic regression, linear discriminant analysis (LDA), and quadratic discriminant analysis QDA. To describe a simple playbook: when the true decision boundaries are linear, then the LDA and logistic regression approaches will tend to perform well. When the boundaries are moderately non-linear, QDA may give better results. Finally, for much more complicated decision boundaries, a non-parametric approach such as KNN can be superior. But the level of smoothness for a non-parametric approach must be chosen carefully.</p>
<p>The logistic regression and LDA methods are closely connected in their linearity but differ in fitting procedures and their assumptions about the distributions in the data. To understand how, consider the two-class setting with <span class="math inline">\(p = 1\)</span> predictor, and let <span class="math inline">\(p_1(x)\)</span> and <span class="math inline">\(p_2(x)=1-p1(x)\)</span> be the probabilities that the observation <span class="math inline">\(X = x\)</span> belongs to class 1 and class 2 respectively. In the LDA framework, the log odds is given by, <span class="math display">\[\log \Bigg( \frac{p_1(x)}{1 -p_1(x)}\Bigg) = \log \Bigg( \frac{p_1(x)}{p_2(x)}\Bigg) = c_0 + c_1 x,\]</span> where <span class="math inline">\(c_0\)</span> and <span class="math inline">\(c_1\)</span> are functions of <span class="math inline">\(\mu_1, \mu_2\)</span>, and <span class="math inline">\(\sigma2\)</span>. In logistic regression we have, <span class="math display">\[\log \Bigg( \frac{p_1(x)}{1 -p_1(x)}\Bigg) = \beta_0 + \beta_1 x.\]</span> Both of these equations are linear functions of x so both logistic regression and LDA produce linear decision boundaries. The only difference is the fact that <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are estimated using maximum likelihood, whereas <span class="math inline">\(c_0\)</span> and <span class="math inline">\(c_1\)</span> are computed using the estimated mean and variance from a normal distribution. This connection also holds for multidimensional data with <span class="math inline">\(p &gt; 1\)</span>.</p>
<p>Though logistic regression and LDA differ only in their fitting procedures, one may outperform the other depending on whether the LDA distribution assumptions are met, i.e. that observations are drawn from a Gaussian distribution with a common covariance matrix.</p>
<p>Recall that KNN is non-parametric approach: no assumptions are made about the shape of the decision boundary. Therefore, we can expect this approach to dominate LDA and logistic regression when the decision boundary is highly non-linear. On the other hand, KNN does not tell us which predictors are important; we don’t get a table of coefficients.</p>
<p>Finally, QDA serves as a compromise between the non-parametric KNN method and the linear LDA and logistic regression approaches. QDA can accurately model a wider range of problems than the linear methods, and though it’s not as flexible as KNN, it can perform better in the presence of a limited number of training observations because it does make some assumptions about the form of the decision boundary.</p>
<h1 id="resampling-methods">Resampling Methods</h1>
<p>Resampling methods involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model. Resampling approaches can be computationally expensive, because they involve fitting the same statistical method multiple times using different subsets of the training data. However, due to recent advances in computing power, the computational requirements of resampling methods generally are not prohibitive. Two of the most commonly used resampling methods are <em>cross-validation</em> and the <em>bootstrap</em>.</p>
<p>Cross-validation can be used to estimate the test error associated with a given statistical learning method, or to select the appropriate level of flexibility. The process of evaluating a model’s performance is known as <em>model assessment</em>, whereas the process of selecting the proper level of flexibility for a model is known as <em>model selection</em>. The bootstrap is most commonly used to provide a measure of accuracy of a parameter estimate or of a given statistical learning method.</p>
<h2 id="cross-validation">Cross-Validation</h2>
<p>In the absence of a very large designated test set that can be used to directly estimate the test error rate some methods make a mathematical adjustment to the training error rate in order to estimate the test error rate, examined later. Alternatively, we now consider a class of methods that estimate the test error rate by holding out a subset of the training observations from the fitting process, and then applying the statistical learning method to those held out observations.</p>
<h3 id="the-validation-set-approach">The Validation Set Approach</h3>
<p>The <em>validation set approach</em> is a very simple strategy that involves randomly dividing the available set of observations into two parts, a <em>training set</em> and a <em>validation set</em> or hold-out set. The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set which provides an estimate of the test error rate.</p>
<p>The validation set approach has two potential drawbacks which will be addressed in cross-validation:</p>
<ol type="1">
<li><p>The validation estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.</p></li>
<li><p>In the validation approach, only a subset of the observations that are included in the training set are used to fit the model. This suggests that the validation set error rate may tend to overestimate the test error rate.</p></li>
</ol>
<h3 id="leave-one-out-cross-validation">Leave-One-Out Cross-Validation</h3>
<p><em>Leave-one-out cross-validation</em> (LOOCV) also involves splitting the set of observations into two parts but instead of creating two subsets of comparable size, a single observation <span class="math inline">\((x_1, y_1)\)</span> is used for the validation set, and the remaining observations <span class="math inline">\(\{(x_2, y_2),...,(x_n, y_n)\}\)</span> make up the training set.</p>
<p>Although <span class="math inline">\(MSE_1 = (y_1 - \hat y_1)^2\)</span> provides an approximately unbiased estimate for the test error, it is a poor estimate because it is highly variable since it is based upon a single observation. We can repeat the procedure by selecting <span class="math inline">\((x_2, y_2)\)</span> for the validation data, training the statistical learning procedure on the other <span class="math inline">\(n - 1\)</span> observations, repeating this approach <span class="math inline">\(n\)</span> times. The LOOCV estimate for the test MSE is the average of these <span class="math inline">\(n\)</span> test error estimates: <span class="math display">\[CV_{(n)} = \frac{1}{n} \sum_{i=1}^n MSE_i.\]</span> Using all but one of the observations for training results in less bias and tends not to overestimate the test error rate as much as the validation set approach does. in contrast to the validation approach, performing LOOCV multiple times will always yield the same results: there is no randomness in the training/validation set splits.</p>
<p>LOOCV can be expensive and time consuming if <span class="math inline">\(n\)</span> is large or the model is slow to fit since the model has to be fit <span class="math inline">\(n\)</span> times. With least squares linear or polynomial regression, an amazing shortcut makes the cost of LOOCV the same as that of a single model fit. <span class="math display">\[CV_{(n)} = \frac{1}{n} \sum_{i=1}^n \Bigg ( \frac{y_i-\hat y_i}{1-h_i} \Bigg )^2.\]</span> where <span class="math inline">\(\hat y_i\)</span> is the <span class="math inline">\(i\)</span>th fitted value from the original least squares fit, and <span class="math inline">\(h_i\)</span> is the leverage defined previously <a href="#eq:leverage" data-reference-type="eqref" data-reference="eq:leverage">[eq:leverage]</a>. This is like the ordinary MSE, except the ith residual is divided by <span class="math inline">\(1 - h_i\)</span> The leverage lies between <span class="math inline">\(1/n\)</span> and <span class="math inline">\(1\)</span>, and reflects the amount that an observation influences its own fit. Hence the residuals for high-leverage points are inflated in this formula by exactly the right amount for this equality to hold. LOOCV is a very general method, and can be used with any kind of predictive modeling.</p>
<h3 id="k-fold-cross-validation">k-Fold Cross-Validation</h3>
<p><em>k-fold CV</em> involves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining <span class="math inline">\(k - 1\)</span> folds, then the mean squared error is computed on the held-out fold. This is repeated <span class="math inline">\(k\)</span> times on different groups of observations. <span class="math display">\[CV_{(k)} = \frac{1}{k} \sum_{i=1}^k MSE_i.\]</span> LOOCV is a special case of k-fold CV in which <span class="math inline">\(k\)</span> equals <span class="math inline">\(n\)</span>. In practice, one typically performs k-fold CV using k = 5 or k = 10 depending on computational costs and the bias-variance trade-off. The actual estimate of the test MSE is may not be of interest, and we’re instead interested in only in the location of the minimum point in the estimated test MSE curve corresponding to the correct flexibility level.</p>
<h3 id="bias-variance-trade-off-for-k-fold-cross-validation">Bias-Variance Trade-Off for k-Fold Cross-Validation</h3>
<p>A more important advantage of k-fold CV is that it often gives more accurate estimates of the test error rate than does LOOCV. This has to do with a bias-variance trade-off. Since LOOCV trains on <span class="math inline">\(n-1\)</span> observations, it seems preferable to k-fold CV from the perspective of bias reduction. However, we must also consider the procedure’s variance. It turns out that LOOCV has higher variance than does k-fold CV with <span class="math inline">\(k&lt;n\)</span>.</p>
<p>When we perform LOOCV, we are in effect averaging the outputs of <span class="math inline">\(n\)</span> fitted models, each of which is trained on an almost identical set of observations; therefore, these outputs are highly (positively) correlated with each other. In contrast, with k-fold CV we are averaging the outputs of k fitted models that are somewhat less correlated with each other, since the overlap between the training sets in each model is smaller. Since the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance.</p>
<h3 id="cross-validation-on-classification-problems">Cross-Validation on Classification Problems</h3>
<p>Cross-validation can also be a very useful approach in the classification setting when <span class="math inline">\(Y\)</span> is qualitative. Rather than using MSE to quantify test error, we instead use the number of misclassified observations. The LOOCV error rate becomes, <span class="math display">\[CV_{(n)} = \frac{1}{n}\sum_{i=1}^n Err_i,\]</span> where <span class="math inline">\(Err_i = I(y_i \neq \hat y_i)\)</span>. The k-fold CV error rate and validation set error rates are defined analogously.</p>
<h2 id="the-bootstrap">The Bootstrap</h2>
<p>The <em>bootstrap</em> can be used to quantify the uncertainty associated with a given estimator or statistical learning method, some for which a measure of variability is otherwise difficult to obtain and is not automatically output by statistical software.</p>
<p>the bootstrap approach allows us to use a computer to emulate the process of obtaining new sample sets. s. Rather than repeatedly obtaining independent data sets from the population, we instead obtain distinct data sets by repeatedly sampling observations from the original data set.</p>
<p>Given a simple data set <span class="math inline">\(Z\)</span> that contains <span class="math inline">\(n\)</span> observations, We randomly select <span class="math inline">\(n\)</span> observations from the data set in order to produce a bootstrap data set, <span class="math inline">\(Z^{*1}\)</span>. The sampling is performed with <em>replacement</em>, which means that the same observation can occur more than once. This procedure is repeated <span class="math inline">\(B\)</span> times for some large value of <span class="math inline">\(B\)</span>, in order to produce <span class="math inline">\(B\)</span> different bootstrap data sets, <span class="math inline">\(Z^{*1}, Z^{*2},\dots,Z^{*B}\)</span> and <span class="math inline">\(B\)</span> corresponding <span class="math inline">\(\alpha\)</span> estimates, <span class="math inline">\(\hat \alpha^{*1},\dots, \hat \alpha^{*B}\)</span>. We can compute the standard error of these bootstrap estimates using the formula, <span class="math display">\[\text{SE}_B(\hat \alpha) = \sqrt{\frac{1}{B-1} \sum_{r=1}^{B} \Bigg ( \hat \alpha ^{*r} - \frac{1}{B} \sum_{r&#39;=1}^{B} \hat \alpha^{*r&#39;} \Bigg )^2 } .\]</span></p>
<h1 id="linear-model-selection-and-regularization">Linear Model Selection and Regularization</h1>
<p>Alternative fitting procedures to least squares can yield better prediction accuracy and model interpretability.</p>
<p>Prediction Accuracy – If <span class="math inline">\(n\)</span>, the number of observations, is much larger than <span class="math inline">\(p\)</span>, the number of variables, i.e. <span class="math inline">\(n &gt;&gt;p\)</span>, then the least squares estimates tend to have low variance and will perform well on test observations. If <span class="math inline">\(n\)</span> is not much larger than <span class="math inline">\(p\)</span>, then there can be a lot of variability in the least squares fit, resulting in overfitting and poor predictions on unseen future observations. If <span class="math inline">\(p&gt;n\)</span>, then there is no longer a unique least squares coefficient estimate: the variance is infinite so the method cannot be used at all.</p>
<p>Model Interpretability – Often some or many of the variables used in a multiple regression model are not associated with the response and their inclusion leads to unnecessary complexity in the resulting model. By setting the corresponding coefficient estimates to zero, we can obtain a model that is more easily interpreted. However, least squares is extremely unlikely to yield any coefficient estimates that are exactly zero. There exist alternative approaches for automatically performing <em>feature selection</em> or <em>variable selection</em>.</p>
<ol type="1">
<li><p>Subset Selection – This approach involves identifying a subset of the p predictors that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables.</p></li>
<li><p>Shrinkage – This approach involves fitting a model involving all p predictors. However, the estimated coefficients are shrunken towards zero relative to the least squares estimates which has the effect of reducing variance. Some coefficients may be estimated to be exactly zero, resulting in variable selection.</p></li>
<li><p>Dimension Reduction – This approach involves projecting the <span class="math inline">\(p\)</span> predictors into a M-dimensional subspace, where <span class="math inline">\(M&lt;p\)</span>. This is achieved by computing <span class="math inline">\(M\)</span> different linear combinations, or projections, of the variables. Then these <span class="math inline">\(M\)</span> projections are used as predictors to fit a linear regression model by least squares.</p></li>
</ol>
<h2 id="subset-selection">Subset Selection</h2>
<h3 id="best-subset-selection">Best Subset Selection</h3>
<p>To perform <em>best subset selection</em>, we fit a separate least squares regression for each possible combination of the <span class="math inline">\(p\)</span> predictors. That is, we fit all <span class="math inline">\(p\)</span> models that contain exactly one predictor, then all <span class="math inline">\(\binom{p}{2}= p(p-1)/2\)</span> models that contain exactly two predictors, and so forth up to <span class="math inline">\(\binom{p}{2}\)</span> . We then try to identify the one that is best model out of the <span class="math inline">\(2^p\)</span> possibilities. Here best is defined as having the smallest RSS, or equivalently largest <span class="math inline">\(R^2\)</span>.</p>
<p>The algorithm is as follows:</p>
<ol type="1">
<li><p>Let <span class="math inline">\(M_0\)</span> denote the null model, which contains no predictors. This model simply predicts the sample mean for each observation.</p></li>
<li><p>For <span class="math inline">\(k = 1, 2,...p\)</span>:</p>
<ol type="1">
<li><p>Fit all <span class="math inline">\(\binom{p}{k}\)</span> models that contain exactly <span class="math inline">\(k\)</span> predictors.</p></li>
<li><p>Pick the best among these <span class="math inline">\(\binom{p}{k}\)</span> models, and call it <span class="math inline">\(M_k\)</span>. Here best is defined as having the smallest RSS, or equivalently largest <span class="math inline">\(R^2\)</span>.</p></li>
</ol></li>
<li><p>Select a single best model from among <span class="math inline">\(M_0,\dots,M_p\)</span> using cross-validated prediction error, <span class="math inline">\(C_p\)</span> (AIC), BIC, or adjusted <span class="math inline">\(R^2\)</span>.</p></li>
</ol>
<p>As the number of features included in the models increases, the RSS of these <span class="math inline">\(p + 1\)</span> models decreases monotonically, and the <span class="math inline">\(R^2\)</span> increases monotonically which indicates a model with a low training error, whereas we wish to choose a model that has a low test error. So if we use these statistics to select the best model, then we will always end up with a model involving all of the variables. This is why in in Step 3, we use cross-validated prediction error in order to select among the models.</p>
<p>The same ideas used with least squares applies to other types of models, such as logistic regression, where instead of ordering models by RSS in Step 2, we use the <em>deviance</em>, a measure that plays the role of RSS for a broader class of models. The deviance is negative two times the maximized log-likelihood; the smaller the deviance, the better the fit.</p>
<p>Best subset selection becomes computationally infeasible for values of <span class="math inline">\(p\)</span> greater than around 40.</p>
<h3 id="stepwise-selection">Stepwise Selection</h3>
<p>An enormous search space can lead to overfitting and high variance of the coefficient estimates which makes <em>stepwise</em> methods, which explore a far more restricted set of models, an attractive alternative to best subset selection.</p>
<h3 id="forward-stepwise-selection" class="unnumbered">Forward Stepwise Selection</h3>
<p>Forward stepwise selection considers a much smaller set of models. begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model. In particular, at each step the variable that gives the greatest additional improvement to the fit is added to the model.</p>
<ol type="1">
<li><p>Let <span class="math inline">\(M_0\)</span> denote the null model, which contains no predictors.</p></li>
<li><p>For <span class="math inline">\(k = 1, 2,...p\)</span>:</p>
<ol type="1">
<li><p>Consider all <span class="math inline">\(p - k\)</span> models that augment the predictors in <span class="math inline">\(M_k\)</span> with one additional predictor.</p></li>
<li><p>Choose the best among these <span class="math inline">\(p - k\)</span> models, and call it <span class="math inline">\(M_{k+1}\)</span>. Here best is defined as having smallest RSS or highest <span class="math inline">\(R^2\)</span>.</p></li>
</ol></li>
<li><p>Select a single best model from among <span class="math inline">\(M_0,\dots,M_p\)</span> using cross-validated prediction error, <span class="math inline">\(C_p\)</span> (AIC), BIC, or adjusted <span class="math inline">\(R^2\)</span>.</p></li>
</ol>
<p>This amounts to a total of <span class="math inline">\(1 +\sum_{k=0}^{p-1} (p-k) = 1 + p(p-1)/2\)</span> models, though it is not guaranteed to find the best possible model out of all <span class="math inline">\(2^p\)</span> models containing subsets of the <span class="math inline">\(p\)</span> predictors. In Step 3, we must identify the best model among a set of models with different numbers of variables, presenting some challenges.</p>
<p>Forward stepwise selection can be applied even in the high-dimensional setting where <span class="math inline">\(n&lt;p\)</span>; however, in this case, it is possible to construct submodels <span class="math inline">\(M_0,\dots,M_{n-1}\)</span> only, since each submodel is fit using least squares, which will not yield a unique solution if <span class="math inline">\(p \geq n\)</span>.</p>
<h3 id="backward-stepwise-selection" class="unnumbered">Backward Stepwise Selection</h3>
<p>Unlike forward stepwise selection, <em>backward stepwise selection</em> begins with the full least squares model containing all <span class="math inline">\(p\)</span> predictors, and then iteratively removes the least useful predictor, one-at-a-time.</p>
<ol type="1">
<li><p>Let <span class="math inline">\(M_0\)</span> denote the null model, which contains no predictors.</p></li>
<li><p>For <span class="math inline">\(k = p, p-1,...1\)</span>:</p>
<ol type="1">
<li><p>Consider all <span class="math inline">\(k\)</span> models that contain all but one of the predictors in <span class="math inline">\(M_k\)</span>, for a total of <span class="math inline">\(k-1\)</span> predictors.</p></li>
<li><p>Choose the best among these <span class="math inline">\(k\)</span> models, and call it <span class="math inline">\(M_{k+1}\)</span>. Here best is defined as having smallest RSS or highest <span class="math inline">\(R^2\)</span>.</p></li>
</ol></li>
<li><p>Select a single best model from among <span class="math inline">\(M_0,\dots,M_p\)</span> using cross-validated prediction error, <span class="math inline">\(C_p\)</span> (AIC), BIC, or adjusted <span class="math inline">\(R^2\)</span>.</p></li>
</ol>
<p>This produces the same number of models as forward selection, i.e. <span class="math inline">\(1 +\sum_{k=0}^{p-1} (p-k)\)</span>, and is also not guaranteed to yield the best model containing a subset of the <span class="math inline">\(p\)</span> predictors.</p>
<h3 id="hybrid-approaches" class="unnumbered">Hybrid Approaches</h3>
<p>Hybrid versions of forward and backward stepwise selection are available, in which variables are added to the model sequentially, as in forward selection, and after adding each new variable, the method may also remove any variables that no longer provide an improvement in the model fit, as in backward selection.</p>
<h3 id="choosing-the-optimal-model">Choosing the Optimal Model</h3>
<p>If we wish to choose a model with a low test error, the training error can be a poor estimate of the test error. Therefore, RSS and <span class="math inline">\(R^2\)</span> are not suitable for selecting the best mode among a collection of models with different numbers of predictors. There are two common approaches to estimate this test error:</p>
<ol type="1">
<li><p>We can indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting.</p></li>
<li><p>We can directly estimate the test error, using either a validation set approach or a cross-validation approach.</p></li>
</ol>
<h3 id="c_p-aic-bic-and-adjusted-r2" class="unnumbered"><span class="math inline">\(C_p\)</span>, AIC, BIC, and Adjusted <span class="math inline">\(R^2\)</span></h3>
<p>Using least squares, we estimate the regression coefficients such that the training RSS (but not the test RSS) is as small as possible. So the training error will decrease as more variables are included in the model, but the test error may not. We now consider four alternative approaches: <span class="math inline">\(C_p\)</span>, <em>Akaike information criterion</em> (AIC), <em>Bayesian information criterion</em> (BIC), and <em>adjusted <span class="math inline">\(R^2\)</span></em>.</p>
<p>For a fitted least squares model containing d predictors, the <span class="math inline">\(C_p\)</span> estimate of test MSE is computed using the equation, <span class="math display">\[C_p = \frac{1}{n}(RSS + 2d\hat\sigma^2)\]</span> where <span class="math inline">\(\hat\sigma2\)</span> is an unbiased estimate of the variance of the error <span class="math inline">\(\epsilon\)</span> associated with each response measurement in the standard linear model. Essentially, the <span class="math inline">\(C_p\)</span> statistic adds a penalty of <span class="math inline">\(2d\hat\sigma^2\)</span> to the training RSS in order to adjust for the fact that the training error tends to underestimate the test error, where the penalty increases as the number of predictors in the model increases. When determining which of a set of models is best, we choose the model with the lowest <span class="math inline">\(C_p\)</span> value.</p>
<p>The AIC criterion is defined for a large class of models fit by maximum likelihood In the case of the model with Gaussian errors, maximum likelihood and least squares are the same thing. In this case AIC is given by</p>
<p><span class="math display">\[AIC = \frac{1}{n\hat\sigma^2} (RSS + 2d\hat\sigma^2)\]</span></p>
<p>BIC is derived from a Bayesian point of view, but ends up looking similar to <span class="math inline">\(C_p\)</span> (and AIC) as well. For the least squares model with <span class="math inline">\(d\)</span> predictors, the BIC is, up to irrelevant constants, given by <span class="math display">\[BIC = \frac{1}{n \hat\sigma^2} (RSS  + \log(n)d\hat\sigma^2)\]</span> Like <span class="math inline">\(C_p\)</span>, we select the model that has the lowest BIC value. Since <span class="math inline">\(\log n &gt; 2\)</span> for any <span class="math inline">\(n &gt; 7\)</span>, the BIC statistic generally places a heavier penalty on models with many variables.</p>
<p>The adjusted <span class="math inline">\(R^2\)</span> statistic is another popular approach for selecting among a set of models that contain different numbers of variables. Recall the usual <span class="math inline">\(R^2\)</span> is defined as <span class="math inline">\(1 - RSS/TSS\)</span>, which always decreases as more variables are added to the mode. For a least squares model with <span class="math inline">\(d\)</span> variables, the adjusted <span class="math inline">\(R^2\)</span> statistic is calculated as, <span class="math display">\[\text{Adjusted }R^2 = 1 - \frac{RSS/(n-d-1)}{TSS/(n-1)}\]</span> Here, a large value of adjusted <span class="math inline">\(R^2\)</span> indicates a model with a small test error. The intuition behind the adjusted R2 is that once all of the correct variables have been included in the model, adding additional noise variables will increase d and increase the denominater while only leading to only a very small decrease in RSS.</p>
<h3 id="validation-and-cross-validation" class="unnumbered">Validation and Cross-Validation</h3>
<p>AIC, BIC, and <span class="math inline">\(C_p\)</span> can also be defined for more general types of models. As an alternative to the approaches just discussed, We can compute the validation set error or the cross-validation error for each model under consideration, and then select the model for which the resulting estimated test error is smallest. This has the advantage of providing a direct estimate of the test error, and makes fewer assumptions about the true underlying model. It can also be used in a wider range of model selection tasks where it is hard to pinpoint the model degrees of freedom i.e. predictors, or to estimate the error variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>In the <em>one-standard-error rule</em>, we first calculate the standard error of the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve. The rationale here is that if a set of models appear to be more or less equally good, then we might as well choose the simplest model—that is, the model with the smallest number of predictors.</p>
<h2 id="shrinkage-methods">Shrinkage Methods</h2>
<p>As an alternative, we can fit a model containing all <span class="math inline">\(p\)</span> predictors using a technique that constrains or regularizes the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero, which can significantly reduce their variance. The two best-known techniques for shrinking the regression coefficients towards zero are ridge regression and the lasso</p>
<h3 id="ridge-regression">Ridge Regression</h3>
<p>Recall the least squares fitting procedure estimates coefficients <span class="math inline">\(\beta_j\)</span> that minimize the RSS <a href="#eq:RSS" data-reference-type="eqref" data-reference="eq:RSS">[eq:RSS]</a>. Similarly, <em>Ride Regression</em> estimates the coefficients <span class="math inline">\(B^R\)</span> that minimize, <span class="math display">\[RSS + \lambda + \lambda \sum_{j=1}^{p} \beta_j^2  
    = \sum_{i=1}^{n} \Bigg( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \Bigg )^2  
    + \lambda \sum_{j=1}^{p} \beta_j^2,\]</span> where <span class="math inline">\(\lambda \geq 0\)</span> is a <em>tuning parameter</em>, to be determined separately. the second term <span class="math inline">\(\lambda \sum \beta_j^2\)</span>, called a <em>shrinkage penalty</em>, is small when <span class="math inline">\(\beta_1,\dots,\beta_p\)</span> are close to zero, and so it has the effect of shrinking the estimates of <span class="math inline">\(\beta_j\)</span> towards zero. The tuning parameter <span class="math inline">\(\lambda\)</span> serves to control the relative impact of these two terms on the regression coefficient estimates excluding the intercept coefficient <span class="math inline">\(\beta_0\)</span>. Ridge regression will produce a different set of coefficient estimates, <span class="math inline">\(\beta^R_\lambda\)</span>, for each value of <span class="math inline">\(\lambda\)</span>, so selecting a good value tuning parameter via cross validation is critical.</p>
<p>The value <span class="math inline">\(X_j\beta^R_{j,\lambda}\)</span> will not only depend on the value of <span class="math inline">\(\lambda\)</span>, but also on the scaling of the <span class="math inline">\(j\)</span>th predictor. In fact, the value of <span class="math inline">\(X_j\beta^R_{j,\lambda}\)</span> may even depend on the scaling of the other predictors. Therefore, it is best to apply ridge regression after standardizing the predictors by applying the estimated standard deviation of the <span class="math inline">\(j\)</span>th predictor with the formula, <span class="math display">\[\Tilde{x_{ij}} = \frac{x_{ij}}{\sqrt{\frac{1}{n} \sum{i=1}^{n} (x_{ij} - \bar x_j)^2}}.\]</span> Ridge regression’s advantage over least squares is rooted in the bias-variance trade-of. As <span class="math inline">\(\lambda\)</span> increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias. This leads to a decrease in the mean squared error (MSE), which is a function of the variance plus the squared bias.</p>
<p>When the number of variables <span class="math inline">\(p\)</span> is almost as large as the number of observations <span class="math inline">\(n\)</span>, the least squares estimates will be extremely variable. And if <span class="math inline">\(p&gt;n\)</span>, then the least squares estimates do not even have a unique solution, whereas ridge regression can still perform well by trading off a small increase in bias for a large decrease in variance. Hence, ridge regression works best in situations where the least squares estimates have high variance.</p>
<p>Though the penalty can shrink some of the coefficients towards zero, ridge regression will include all <span class="math inline">\(p\)</span> predictors in the final model. This can create a challenge in model interpretation</p>
<h3 id="the-lasso">The Lasso</h3>
<p>The <em>lasso</em> is a relatively recent alternative to ridge regression that overcomes the disadvantage of uninterpretable models with a large number of predictors. The lasso coefficients, <span class="math inline">\(\Tilde{\beta^L_\lambda}\)</span>, minimize the quantity <span class="math display">\[RSS + \lambda \sum_{j=1}^{p} |\beta_j| 
    = \sum_{i=1}^{n} \Bigg( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \Bigg )^2 +  \lambda \sum_{j=1}^{p} |\beta_j| .\]</span> The difference between ridge regression and lasso is the penalty <span class="math inline">\(\beta_j^2\)</span> has been replaced with <span class="math inline">\(|\beta_j|\)</span>. The lasso uses an <span class="math inline">\(\ell_1\)</span> (pronounced "ell 1") penalty instead of an <span class="math inline">\(\ell_2\)</span> penalty. The <span class="math inline">\(\ell_1\)</span> norm of a coefficient vector <span class="math inline">\(\beta\)</span> is given by <span class="math inline">\(||\beta||_1 = \sum |\beta_j|\)</span>. This has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter <span class="math inline">\(\lambda\)</span> is sufficiently large. Thus, the lasso performs variable selection, resulting in more interpretable and <em>sparse</em> models, involving only a subset of the variables.</p>
<h3 id="the-variable-selection-property-of-the-lasso" class="unnumbered">The Variable Selection Property of the Lasso</h3>
<p>When we perform the lasso we are trying to find the set of coefficient estimates that lead to the smallest RSS, subject to the constraint that there is a budget <span class="math inline">\(s\)</span> for how large <span class="math inline">\(\sum_{j=1}^p |\beta_j |\)</span> can be. Ridge regression more or less shrinks every dimension of the data by the same proportion, whereas the lasso more or less shrinks all coefficients toward zero by a similar amount, and sufficiently small coefficients are shrunken all the way to zero, known as <em>soft thresholding</em>.</p>
<h3 id="comparing-the-lasso-and-ridge-regression" class="unnumbered">Comparing the Lasso and Ridge Regression</h3>
<p>Neither ridge regression nor the lasso will universally dominate the other. In general, one might expect the lasso to perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or that equal zero. Ridge regression will perform better when the response is a function of many predictors, all with coefficients of roughly equal size. However, the number of predictors that is related to the response is never known a priori for real data sets. A technique such as cross-validation can be used in order to determine which approach is better on a particular data set.</p>
<h3 id="bayesian-interpretation-for-ridge-regression-and-the-lasso" class="unnumbered">Bayesian Interpretation for Ridge Regression and the Lasso</h3>
<p>A Bayesian viewpoint for regression assumes that the coefficient vector <span class="math inline">\(\beta\)</span> has some prior distribution, say <span class="math inline">\(p(\beta)\)</span>, where <span class="math inline">\(\beta = (\beta_0, \dots, \beta_p )^T\)</span>. The likelihood of the data is <span class="math inline">\(f(Y |X, \beta)\)</span> where <span class="math inline">\(X= (X_1, \dots, X_p)^T\)</span>. Multiplying the prior distribution by the likelihood gives us the <em>posterior distribution</em>, <span class="math display">\[p(\beta|X, Y ) \propto f(Y |X, \beta) p(\beta|X) = f(Y |X, \beta)p(\beta),\]</span> which follows from Bayes’ theorem. We assume the usual linear model, and suppose that the errors are independent and drawn from a normal distribution. We also assume <span class="math inline">\(p(\beta) = \prod^{p}+{j=1} g(\beta_j)\)</span>, for some density function <span class="math inline">\(g\)</span>. Then, ridge regression and the lasso follow from two special cases of <span class="math inline">\(g\)</span>:</p>
<ol type="1">
<li><p>If <span class="math inline">\(g\)</span> is a Gaussian distribution with mean zero and standard deviation a function of <span class="math inline">\(\lambda\)</span>, then it follows that the posterior mode for <span class="math inline">\(\beta\)</span>, i.e. the most likely value for <span class="math inline">\(\beta\)</span>, given the data, is given by the ridge regression solution.</p></li>
<li><p>If <span class="math inline">\(g\)</span> is a double-exponential (Laplace) distribution with mean zero and scale parameter a function of <span class="math inline">\(\lambda\)</span>, then it follows that the posterior mode for <span class="math inline">\(\beta\)</span> is the lasso solution.</p></li>
</ol>
<h3 id="selecting-the-tuning-parameter">Selecting the Tuning Parameter</h3>
<p>Implementing ridge regression and the lasso requires a method for selecting a value for the tuning parameter <span class="math inline">\(\lambda\)</span>. Cross-validation provides a simple way to tackle this problem. We choose a grid of <span class="math inline">\(\lambda\)</span> values, and compute the cross-validation error for each value of <span class="math inline">\(\lambda\)</span>. We then select the tuning parameter value for which the cross-validation error is smallest. Finally, the model is re-fit using all of the available observations and the selected value of the tuning parameter.</p>
<h2 id="dimension-reduction-methods">Dimension Reduction Methods</h2>
<p><em>Dimension reduction methods</em> transform the predictors and then fit a least squares model using the transformed variables. Let <span class="math inline">\(Z_1, Z_2,\dots, Z_M\)</span> represent <span class="math inline">\(M&lt;p\)</span> linear combinations of our original <span class="math inline">\(p\)</span> predictors. That is, <span class="math display">\[Z_m = \sum_{j=0}^p \phi_{jm}X_j\]</span> for some constants <span class="math inline">\(\phi_{1m}, \phi_{2m},\dots,\phi_{pm}, m = 1,...,M\)</span>. We can then fit the linear regression model, <span class="math display">\[y_i = \theta_0 + \sum_{m=1}^M \theta_m z_{im} + \epsilon_i, \ \ i = 1,\dots, n\]</span> using least squares, which can possibly outperform the standard linear model with wisely chosen coefficients. The term dimension reduction comes from the fact that this approach reduces the problem of estimating the <span class="math inline">\(p+1\)</span> to <span class="math inline">\(M+1\)</span> coefficients, where <span class="math inline">\(M &lt; p\)</span>. The model can be thought of as a special case of the linear model, with coeffecients <span class="math inline">\(\theta_m\)</span> being related by <span class="math inline">\(\beta_j = \sum_{m=1}^M \theta_m \phi_{jm}\)</span>. Methods for obtaining the tranformed predictors are examined next.</p>
<h3 id="principal-component-regression">Principal Component Regression</h3>
<p><em>Principal components analysis</em> (PCA) is a popular approach for deriving a low-dimensional set of features from a large set of variables and is examined in more detail later. PCA is a technique for reducing the dimension of a <span class="math inline">\(n \times n\)</span> data matrix <span class="math inline">\(X\)</span>.</p>
<p>The first principal component, <span class="math inline">\(Z_1\)</span>, moves along the data in the direction which the observations vary the most, where directions corresponds to linear combinations. Alternatively, we can think of the first principal component as a vector that defines the line that is as close as possible to the data. The first principal component score measures this distance by selecting linear combinations of <span class="math inline">\(X_i, X_j\)</span> predictors with <span class="math inline">\(\sigma_{11}^2 + \sigma_{21}^2 = 1\)</span> that yield the highest variance. With two-dimensional data and two predictors <span class="math inline">\(X_1, X_2\)</span>, this looks like, <span class="math display">\[Z_1 = Var(\sigma_{11} \times (X_1 - \bar X )  + \sigma_{21} \times (X_2  - \bar X)).\]</span> The second principal component <span class="math inline">\(Z^2\)</span> is a linear combination of the variables that are uncorrelated with <span class="math inline">\(Z^1\)</span>, and has largest variance subject to this constraint, i.e. the direction must be perpendicular, or perpendicular orthogonal, to the first principal component direction. <span class="math display">\[Z_2 = Var(\sigma_{21} \times (X_1 - \bar X )  + \sigma_{11} \times (X_2  - \bar X))\]</span> With more predictors, additional components could be constructed. They would successively maximize variance, subject to the constraint of being uncorrelated with the preceding components.</p>
<h3 id="the-principal-components-regression-approach" class="unnumbered">The Principal Components Regression Approach</h3>
<p>The <em>principal components regression</em> (PCR) approach involves constructing the first <span class="math inline">\(M\)</span> principal components, <span class="math inline">\(Z_1,\dots,Z_M\)</span>, and then using these components as the predictors in a linear regression model that is fit using least squares. Often a small number of principal components suffice to explain most of the variability in the data, as well as the relationship with the response, i.e. the directions in which <span class="math inline">\(X_1,...,X_p\)</span> show the most variation are the directions that are associated with <span class="math inline">\(Y\)</span>. If this assumption holds, our PCR model will lead to better results since using less coefficients will mitigate overfitting. PCR will tend to do well in cases when the first few principal components are sufficient to capture most of the variation in the predictors as well as the relationship with the response.</p>
<p>Yet PCR is not considered a feature selection method, since the <span class="math inline">\(M\)</span> principal components used in the regression is a linear combination of all <span class="math inline">\(p\)</span> of the original features. In this sense, PCR is more closely related to ridge regression than to the lasso, where ridge regression can be thought of as a continuous version of PCR. It’s recommended to standardize each predictor prior to generating the principal components with PCR because high-variance variables will tend to play a larger role in the principal components obtained</p>
<p>The directions or linear combinations are identified in an unsupervised way, since the response <span class="math inline">\(Y\)</span> is not used to help determine or supervise the indentification of the principal component directions. Consequently, PCR suffers from the drawback that there is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response.</p>
<h3 id="partial-least-squares">Partial Least Squares</h3>
<p><em>Partial least squares</em> (PLS) is a supervised alternative to PCR. That is, PLS is also a dimension reduction method which identifies a set of features that are linear combinations of the original features and then fits a linear model via least squares. But unlike PCR, PLS uses a supervised method to identifiy features that not only approximate the old features well, but also that are related to the response by attempting to find directions that help explain both the response and the predictors.</p>
<p>After standardizing the p predictors, PLS computes the first direction <span class="math inline">\(Z_1\)</span> by setting each <span class="math inline">\(\phi_{j1}\)</span> in the dimension reduction features equal to the coefficient from the simple linear regression of <span class="math inline">\(Y\)</span> onto <span class="math inline">\(X_j\)</span>. One can show that this coefficient is proportional to the correlation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_j\)</span>. Hence, in computing <span class="math inline">\(Z_1 = \sum_{j=1}^{p} \phi_{j1}X_j\)</span>, PLS places the highest weight on the variables that are most strongly related to the response.</p>
<p>To identify the second PLS direction we first adjust each of the variables for <span class="math inline">\(Z_1\)</span>, by regressing each variable on <span class="math inline">\(Z_1\)</span> and taking residuals. These residuals can be interpreted as the remaining information that has not been explained by the first PLS direction. We then compute <span class="math inline">\(Z_2\)</span> using this orthogonalized data in exactly the same fashion as <span class="math inline">\(Z_1\)</span> was computed based on the original data. This iterative approach can be repeated <span class="math inline">\(M\)</span> times to identify multiple PLS components <span class="math inline">\(Z_1,\dots,Z_M\)</span>. Finally, at the end of this procedure, we use least squares to fit a linear model to predict <span class="math inline">\(Y\)</span> using <span class="math inline">\(Z_1,\dots,Z_M\)</span> in exactly the same fashion as for PCR. As with PCR, the number <span class="math inline">\(M\)</span> of partial least squares directions used in PLS is a tuning parameter that is typically chosen by cross-validation.</p>
<p>In practice PLS often performs no better than ridge regression or PCR. While the supervised dimension reduction of PLS can reduce bias, it also has the potential to increase variance.</p>
<h2 id="considerations-in-high-dimensions">Considerations in High Dimensions</h2>
<h3 id="high-dimensional-data">High-Dimensional Data</h3>
<p>A <em>low-dimensional</em> setting is one in which <span class="math inline">\(n\)</span>, the number of observations, is much greater than <span class="math inline">\(p\)</span>, the number of features, i.e. <span class="math inline">\(n&gt;&gt;p\)</span>.</p>
<p>Data sets containing more features than observations are often referred to as <em>high-dimensional</em>. Classical approaches such as least squares linear regression are not appropriate in this setting. Challenges like the bias-variance trade-off and the danger of overfitting are still relevant and can become particularly important in this setting.</p>
<h3 id="what-goes-wrong-in-high-dimensions">What Goes Wrong in High Dimensions?</h3>
<p>When the number of features <span class="math inline">\(p\)</span> is as large as, or larger than, the number of observations n, least squares should not be performed. Regardless of whether or not there truly is a relationship between the features and the response, least squares will yield a set of coefficient estimates that result in a perfect fit to the data, such that the residuals are zero. When <span class="math inline">\(p&gt;n\)</span> or <span class="math inline">\(p \approx n\)</span>, a simple least squares regression line is too flexible and hence overfits the data.</p>
<p>Care must be taken when analyzing data sets with a large number of variables, accurate evaluations of models performance must be done on an independent test set, otherwise the training set MSE can create false confidence.</p>
<p>Additionally, techniques visited earlier, like <span class="math inline">\(C_p\)</span>, AIC, and BIC, which were useful for adjusting the training set RSS or <span class="math inline">\(R^2\)</span> in order to account for the number of variables used to fit a least squares model are also not appropriate in the high-dimensional setting. This is because estimating <span class="math inline">\(\hat\sigma^2\)</span> can be problematic and one can easily obtain a model with an adjusted <span class="math inline">\(R^2\)</span> value of 1.</p>
<h3 id="regression-in-high-dimensions">Regression in High Dimensions</h3>
<p>We can avoid overfitting by using a less flexible fitting approach than least squares. Forward stepwise selection, ridge regression, the lasso, and principal components regression, are particularly useful for performing regression in the high-dimensional setting</p>
<p>We find that (1) regularization or shrinkage plays a key role in high-dimensional problems, (2) appropriate tuning parameter selection is crucial for good predictive performance, and (3) the test error tends to increase as the dimensionality of the problem (i.e. the number of features or predictors) increases, unless the additional features are truly associated with the response) this is also known as the curse of dimensionality. noise features increase the dimensionality of the problem, exacerbating the risk of overfitting. Even if excess features are relevant, the variance incurred in fitting their coefficients may outweigh the reduction in bias that they bring.</p>
<h3 id="interpreting-results-in-high-dimensions">Interpreting Results in High Dimensions</h3>
<p>In the high-dimensional setting, the multicollinearity problem is extreme: any variable in the model can be written as a linear combination of all of the other variables in the model. This means that we can never know exactly which variables truly are predictive of the outcome, and we can never identify the best coefficients for use in the regression and can only hope to assign appropriate regression coefficients to correlated variables.</p>
<p>As mentioned before, one should never use sum of squared errors, p-values, <span class="math inline">\(R^2\)</span> statistics or other traditional measures of model fit on the training data as evidence of a good model fit in the high-dimensional setting. It is important to instead report results on an independent test set, or cross-validation errors. For instance, the MSE or <span class="math inline">\(R^2\)</span> on an independent test set is a valid measure of model fit, but the MSE on the training set is not.</p>
<h1 id="moving-beyond-linearity">Moving Beyond Linearity</h1>
<h1 id="tree-based-methods">Tree-based Methods</h1>
<h1 id="support-vector-machines">Support Vector Machines</h1>
<h1 id="unsupervised-learning">Unsupervised Learning</h1></li>
</ol>
<p>Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. An Introduction to Statistical Learning : with Applications in R. New York :Springer, 2013.</p>
<p>http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf</p>
<p>https://arxiv.org/pdf/1010.3162.pdf</p>
<p>http://bactra.org/notebooks/learning-theory.html</p>
</body>
</html>
