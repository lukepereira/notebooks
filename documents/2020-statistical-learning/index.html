<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Statistical Learning Notes (ISLR)</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Statistical Learning Notes (ISLR)</h1>
</header>
<ul>
<li><a href="#introduction">Introduction</a>
<ul>
<li><a href="#a-brief-history-of-statistical-learning">A Brief History of Statistical Learning</a></li>
</ul></li>
<li><a href="#statistical-learning">Statistical Learning</a>
<ul>
<li><a href="#what-is-statistical-learning">What is Statistical Learning</a>
<ul>
<li><a href="#why-estimate-f">Why Estimate <span class="math inline">\(f\)</span>?</a></li>
<li><a href="#prediction">Prediction</a></li>
<li><a href="#inference">Inference</a></li>
<li><a href="#how-do-we-estimate-f">How Do We Estimate <span class="math inline">\(f\)</span>?</a></li>
<li><a href="#parametric-methods">Parametric Methods</a></li>
<li><a href="#non-parametric-methods">Non-parametric Methods</a></li>
<li><a href="#the-trade-off-between-prediction-accuracy-and-model-interpretability">The Trade-Off Between Prediction Accuracy and Model Interpretability</a></li>
<li><a href="#supervised-versus-unsupervised-learning">Supervised Versus Unsupervised Learning</a></li>
<li><a href="#regression-versus-classification-problems">Regression Versus Classification Problems</a></li>
</ul></li>
<li><a href="#assessing-model-accuracy">Assessing Model Accuracy</a>
<ul>
<li><a href="#measuring-the-quality-of-fit">Measuring the Quality of Fit</a></li>
<li><a href="#the-bias-variance-trade-off">The Bias-Variance Trade-Off</a></li>
<li><a href="#the-classification-setting">The Classification Setting</a></li>
<li><a href="#the-bayes-classifier">The Bayes Classifier</a></li>
<li><a href="#k-nearest-neighbors">K-Nearest Neighbors</a></li>
</ul></li>
</ul></li>
<li><a href="#linear-regression">Linear Regression</a>
<ul>
<li><a href="#simple-linear-regression">Simple Linear Regression</a>
<ul>
<li><a href="#estimating-the-coefficients">Estimating the Coefficients</a></li>
<li><a href="#assessing-the-accuracy-of-the-coefficient-estimates">Assessing the Accuracy of the Coefficient Estimates</a></li>
<li><a href="#assessing-the-accuracy-of-the-model">Assessing the Accuracy of the Model</a></li>
<li><a href="#residual-error">Residual Error</a></li>
<li><a href="#r2-statistic">R2 Statistic</a></li>
</ul></li>
<li><a href="#multiple-linear-regression">Multiple Linear Regression</a>
<ul>
<li><a href="#estimating-the-regression-coefficients">Estimating the Regression Coefficients</a></li>
<li><a href="#some-important-questions">Some Important Questions</a></li>
<li><a href="#is-there-a-relationship-between-the-response-and-predictors">Is There a Relationship Between the Response and Predictors?</a></li>
<li><a href="#deciding-on-important-variables">Deciding on Important Variables</a></li>
<li><a href="#model-fit">Model Fit</a></li>
<li><a href="#predictions">Predictions</a></li>
</ul></li>
<li><a href="#other-considerations-in-the-regression-model">Other Considerations in the Regression Model</a>
<ul>
<li><a href="#qualitative-predictors">Qualitative Predictors</a></li>
<li><a href="#predictors-with-only-two-levels">Predictors with Only Two Levels</a></li>
<li><a href="#qualitative-predictors-with-more-than-two-levels">Qualitative Predictors with More than Two Levels</a></li>
<li><a href="#extensions-of-the-linear-model">Extensions of the Linear Model</a></li>
<li><a href="#removing-the-additive-assumption">Removing the Additive Assumption</a></li>
<li><a href="#non-linear-relationships">Non-linear Relationships</a></li>
<li><a href="#potential-problems">Potential Problems</a></li>
</ul></li>
</ul></li>
</ul>
<h1 id="introduction">Introduction</h1>
<h2 id="a-brief-history-of-statistical-learning">A Brief History of Statistical Learning</h2>
<ol type="1">
<li><p>At the beginning of the nineteenth century, Legendre and Gauss published papers on the method of least squares, which implemented the earliest form of what is now known as linear regression.</p></li>
<li><p>In 1936 Fisher proposed linear discriminant analysis.</p></li>
<li><p>In the 1940s, various authors put forth an alternative approach, logistic regression.</p></li>
<li><p>In the 1950’s, Frank Rosenblatt introduced the Perceptron and Neural Networks</p></li>
<li><p>In the 1960’s, various authors introduced Nearest Neighbor and K-means clustering.</p></li>
<li><p>In the early 1970s, Nelder and Wedderburn coined the term generalized linear models for an entire class of statistical learning methods that include both linear and logistic regression as special cases.</p></li>
<li><p>By the end of the 1970s, many more techniques for learning from data were available but all were almost all linear because of computational limitations.</p></li>
<li><p>By the 1980s, computing technology had finally improved sufficiently that non-linear methods were no longer computationally prohibitive</p></li>
<li><p>In mid 1980s Breiman, Friedman, Olshen and Stone introduced classification and regression trees, including cross-validation for model selection.</p></li>
<li><p>In 1986, Hastie and Tibshirani introduced generalized additive models for a class of non-linear extensions to generalized linear models.</p></li>
<li><p>In the 1990’s Vapnik introduced Support Vector Machines.</p></li>
<li><p>In the 2000’s Brieman introduced Random Forest</p></li>
<li><p>In the 2000’s Hinton popularized Deep Learning and Artificial Neural Networks.</p></li>
</ol>
<h1 id="statistical-learning">Statistical Learning</h1>
<h2 id="what-is-statistical-learning">What is Statistical Learning</h2>
<p><span class="math inline">\(n\)</span> – number of observations or training data.</p>
<p><span class="math inline">\(p\)</span> – number of features or parameters.</p>
<p><span class="math inline">\(x_{ij}\)</span> – the value of the <span class="math inline">\(j\)</span>th predictor, or input, for observation <span class="math inline">\(i\)</span></p>
<p><span class="math inline">\(X = (x_1, \dots, x_p)\)</span> – input vector a.k.a features, predictors, independent variables.</p>
<p><span class="math inline">\(\epsilon\)</span> – error term, independent of <span class="math inline">\(X\)</span> and has mean zero.</p>
<p><span class="math inline">\(Y = f(X) + \epsilon\)</span> – a model a.k.a. output or dependent variable.</p>
<p><span class="math inline">\(y_i\)</span> – the response variable for the <span class="math inline">\(i\)</span>th observation</p>
<p>In essence, statistical learning refers to a set of approaches for estimating <span class="math inline">\(f\)</span>.</p>
<h3 id="why-estimate-f">Why Estimate <span class="math inline">\(f\)</span>?</h3>
<p>There are two main reasons for estimating <span class="math inline">\(f\)</span>: prediction and inference.</p>
<h3 class="unnumbered" id="prediction">Prediction</h3>
<p>We can predict <span class="math inline">\(Y\)</span> using <span class="math inline">\(\hat Y = \hat f(X)\)</span> where <span class="math inline">\(\hat f\)</span> represents an estimate of <span class="math inline">\(f\)</span> and <span class="math inline">\(\hat Y\)</span> represents the resulting prediction for <span class="math inline">\(Y\)</span>.</p>
<p>The accuracy of <span class="math inline">\(\hat Y\)</span> as a prediction for <span class="math inline">\(Y\)</span> depends on two quantities, the reducible error and the irreducible error.</p>
<p>Reducible error is the result of an inaccurate statistical learning technique.</p>
<p>Irreducible error occurs because <span class="math inline">\(Y\)</span> is a function of <span class="math inline">\(\epsilon\)</span> which has variability that is dependent of <span class="math inline">\(X\)</span>, so cannot be reduced via a statistical learning technique. Irreducible error may be larger than zero because of unmeasured variables or unmeasurable variation. The irreducible error will always provide an upper bound on the accuracy of our prediction for Y and is almost always unknown in practice.</p>
<p><span class="math display">\[E[(Y - \hat f (x))^2 | X = x] = \underbrace{[f(x) - \hat f(x)]^2}_{\text{Reducible}} + \underbrace{Var(\epsilon)}_{\text{Irreducible}}\]</span></p>
<p>Note, <span class="math inline">\(E(Y - \hat Y)^2 = E[(Y - \hat f (x))^2 | X = x] = E[f(X) + \epsilon - \hat f(X)]^2\)</span> represents the average, or expected value, of the squared difference between the predicted and actual value of <span class="math inline">\(Y\)</span> and <span class="math inline">\(Var()\)</span> represents the variance associated with the error term <span class="math inline">\(\epsilon\)</span>. We square the values in order to ignore the resulting sign when finding averages.</p>
<h3 class="unnumbered" id="inference">Inference</h3>
<p>Often, we are interested in understanding how <span class="math inline">\(Y\)</span> changes as a function of <span class="math inline">\(X_1,\dots,X_p\)</span>. That is, when we vary the value of a given feature, what should the result look like. Now <span class="math inline">\(\hat f\)</span> cannot be treated as a black box, because we need to know its exact form.</p>
<p>Common questions occurring in this setting include,</p>
<ul>
<li><p>Which predictors are associated with the response?</p></li>
<li><p>What is the relationship between the response and each predictor?</p></li>
<li><p>Can the relationship between Y and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?</p></li>
</ul>
<p>Depending on whether our ultimate goal is prediction, inference, or a combination of the two, different methods for estimating <span class="math inline">\(f\)</span> may be appropriate though some modeling could be conducted both for prediction and inference.</p>
<p>Linear models allow for relatively simple and interpretable inference, but may not yield as accurate predictions as some other approaches. In contrast, some highly non-linear approaches can potentially provide quite accurate predictions for <span class="math inline">\(Y\)</span> at the expense of a less interpretable model, for which inference is more challenging.</p>
<h3 id="how-do-we-estimate-f">How Do We Estimate <span class="math inline">\(f\)</span>?</h3>
<p>Our training data or observations can be represented as <span class="math inline">\({(x_1, y_1),(x_2, y_2),...,(x_n, y_n)}\)</span> where <span class="math inline">\(x_i = (x_{i_1}, x_{i_2},...,x_{i_p})^T\)</span> are vectors and <span class="math inline">\(y_i\)</span> are typically scalars.</p>
<p>Then, we want to find a function <span class="math inline">\(\hat f\)</span> such that <span class="math inline">\(Y \approx \hat f(X)\)</span> for any observation <span class="math inline">\((X, Y)\)</span></p>
<p>Broadly speaking, most statistical learning methods for this task can be characterized as either parametric or non-parametric</p>
<h3 class="unnumbered" id="parametric-methods">Parametric Methods</h3>
<p>This approach reduces the problem of estimating <span class="math inline">\(f\)</span> down to one of estimating a set of parameters.</p>
<ol type="1">
<li><p>First, we make an assumption about the functional form, or shape, of <span class="math inline">\(f\)</span>.</p></li>
<li><p>After a model has been selected, we need a procedure that uses the training data to fit or train the model.</p></li>
</ol>
<p>For example, if we assume <span class="math inline">\(f\)</span> is linear, then <span class="math inline">\(f(X) = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_p X_p\)</span>. To train the linear model, we need to estimate the parameters <span class="math inline">\(\beta_0, \beta_1,..., \beta_p\)</span>, which is commonly done using (ordinary) least squares.</p>
<p>In general, fitting a more flexible model requires estimating a greater number of parameters. These more complex models can lead to a phenomenon known as <strong>overfitting</strong> the data, which essentially means they follow the errors, or noise, too closely.</p>
<h3 class="unnumbered" id="non-parametric-methods">Non-parametric Methods</h3>
<p>Non-parametric methods do not make explicit assumptions about the functional form of <span class="math inline">\(f\)</span> and instead seek an estimate of <span class="math inline">\(f\)</span> that gets as close to the data points as possible while being reasonably smooth.</p>
<p>A very large number of observations (far more than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for <span class="math inline">\(f\)</span>. In order to fit a thin-plate spline, the data analyst must select a level of smoothness.</p>
<h3 id="the-trade-off-between-prediction-accuracy-and-model-interpretability">The Trade-Off Between Prediction Accuracy and Model Interpretability</h3>
<figure>
<img src="isl-figure-1.png" style="width:11cm" alt="" /><figcaption>image</figcaption>
</figure>
<p>If we are mainly interested in inference, then restrictive models are much more interpretable. In contrast, very flexible approaches, such as the splines and the boosting methods can lead to such complicated estimates of <span class="math inline">\(f\)</span> that it is difficult to understand how any individual predictor is associated with the response. Though there are clear advantages to using simple and relatively inflexible statistical learning methods when inference is the goal, the converse is not typically true. Instead we will often obtain more accurate predictions using a less flexible method. This phenomenon has to do with the potential for overfitting in highly flexible methods.</p>
<h3 id="supervised-versus-unsupervised-learning">Supervised Versus Unsupervised Learning</h3>
<p>Most statistical learning problems fall into one of two categories: supervised or unsupervised.</p>
<p>In supervised learning, for each observation of the predictor measurement(s) <span class="math inline">\(x_i, i = 1,...,n\)</span> there is an associated response measurement <span class="math inline">\(y_i\)</span>. We wish to fit a model that relates the response to the predictors, with the aim of accurately predicting the response for future observations (prediction) or better understanding the relationship between the response and the predictors (inference).</p>
<p>In contrast, unsupervised learning describes the situation in which for every observation <span class="math inline">\(i = 1,...,n\)</span>, we observe a vector of measurements <span class="math inline">\(x_i\)</span> but no associated response <span class="math inline">\(y_i\)</span>. It is not possible to fit a linear regression model, since there is no response variable to predict. One statistical learning tool that we may use in this setting is cluster analysis which aims to ascertain, on the basis of <span class="math inline">\(x_1,..., x_n\)</span>, whether the observations fall into relatively distinct groups.</p>
<p>In a semi-supervised learning problem, we wish to use a statistical learning method that can incorporate the <span class="math inline">\(m\)</span> observations for which response measurements are available as well as the <span class="math inline">\(n - m\)</span> observations for which they are not.</p>
<h3 id="regression-versus-classification-problems">Regression Versus Classification Problems</h3>
<p>Variables can be characterized as either quantitative (taking on numerical values) or qualitative/categorical. We tend to refer to problems with a quantitative response as regression problems, while those involving a qualitative response are often referred to as classification problems, but it’s not always clear-cut and many problems can use either responses. Whether the predictors are qualitative or quantitative is generally considered less important provided that any qualitative predictors are properly coded before the analysis is performed.</p>
<h2 id="assessing-model-accuracy">Assessing Model Accuracy</h2>
<h3 id="measuring-the-quality-of-fit">Measuring the Quality of Fit</h3>
<p>In order to evaluate the performance of a statistical learning method on a given data set, we need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation.</p>
<p>The most commonly-used measure is the mean squared error (MSE), given by <span class="math display">\[MSE = \frac{1}{n} \sum_{i=1}^n (y_i - \hat f(x_i))^2.\]</span></p>
<p>The MSE computed using the training data that was used to fit the model can be referred to as the training MSE. We want to choose the method that gives the lowest test MSE, i.e. we want <span class="math inline">\(\hat f(x_0)\)</span> to be approximately equal to <span class="math inline">\(y_0\)</span>, where <span class="math inline">\((x_0, y_0)\)</span> is a previously unseen test observation not used to train the statistical learning method.</p>
<p>As model flexibility increases, training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. This is a fundamental property of statistical learning that holds regardless of the particular data set at hand and regardless of the statistical method being used. When we overfit the training data, the test MSE will be very large because the supposed patterns that the method found in the training data simply don’t exist in the test data.</p>
<p>One important method for estimating test MSE using the training data is cross-validation.</p>
<h3 id="the-bias-variance-trade-off">The Bias-Variance Trade-Off</h3>
<p>The expected test MSE, for a given value <span class="math inline">\(x_0\)</span>, can always be decomposed into the sum of three fundamental quantities: the variance of <span class="math inline">\(\hat f(x0)\)</span>, the squared bias of <span class="math inline">\(\hat f(x_0)\)</span> and the variance of the error variance bias terms. That is,</p>
<p><span class="math display">\[E \big (y_0 - \hat f(x_0) \big )^2 = Var( \hat f(x_0)) + [Bias( \hat f(x_0))]^2 + Var(\epsilon).\]</span></p>
<p><span class="math inline">\(E \big (y_0 - \hat f(x_0) \big )^2\)</span> defines the expected test MSE, computed by averaging the term over all possible values of <span class="math inline">\(x_0\)</span> in the test set.</p>
<p>In order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves low variance and low bias.</p>
<p>Variance refers to the amount by which <span class="math inline">\(\hat f\)</span> would change if we estimated it using a different training data set. In general, more flexible statistical methods have higher variance. This is because changing any one of the data points may cause the estimate <span class="math inline">\(\hat f\)</span> to change considerably. A model with high variance does not generalize on the data which it hasn’t seen before.</p>
<p>Bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. Generally, more flexible methods result in less bias</p>
<p>As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. The relative rate of change of these two quantities determines whether the test MSE increases or decreases. As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the expected test MSE declines. However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases.</p>
<p>The relationship between bias, variance, and test set MSE is referred to as the bias-variance trade-off.</p>
<figure>
<img src="bias-variance-tradeoff.png" style="width:13cm" alt="" /><figcaption>image</figcaption>
</figure>
<h3 id="the-classification-setting">The Classification Setting</h3>
<p>Many of the previous concepts, including the bias-variance trade-off, transfer over to the classification setting with only some modifications due to the fact that <span class="math inline">\(y_i\)</span> is no longer numerical. Suppose that we seek to estimate <span class="math inline">\(f\)</span> on the basis of training observations <span class="math inline">\(\{(x_1, y_1),...,(x_n, y_n)\}\)</span>, where now <span class="math inline">\(y_1,...,y_n\)</span> are qualitative.</p>
<p>The most common approach for quantifying the accuracy of our estimate <span class="math inline">\(\hat f\)</span> is the <em>training error rate</em>, the proportion of mistakes that are made if we apply error rate our estimate <span class="math inline">\(\hat f\)</span> to the training observations:</p>
<p><span class="math display">\[\frac{1}{n} \sum_{i=1}^n I(y_i  \neq \hat y_i)\]</span></p>
<p>Here <span class="math inline">\(\hat y_i\)</span> is the predicted class label for the <span class="math inline">\(i\)</span>th observation using <span class="math inline">\(\hat f\)</span>. And <span class="math inline">\(I(y_i \neq \hat y_i)\)</span> is an indicator variable that equals 1 if the <span class="math inline">\(i\)</span>th observation was misclassified, i.e. <span class="math inline">\(y_i \neq \hat y_i\)</span>, and zero it was was classified correctly, i.e.<span class="math inline">\(y_i = \hat y_i\)</span>.</p>
<p>The <em>test error rate</em> associated with a set of test observations of the form <span class="math inline">\((x_0, y_0)\)</span> is given by <span class="math display">\[Ave(I(y_0 \neq \hat y_0)),\]</span> where <span class="math inline">\(\hat y_0\)</span> is the predicted class label. A good classifier minimizes the test error.</p>
<h3 id="the-bayes-classifier">The Bayes Classifier</h3>
<p>To minimize the test error rate, on average, we should simply assign a test observation with predictor vector <span class="math inline">\(x_0\)</span> to the class <span class="math inline">\(j\)</span> for which the conditional probability <span class="math display">\[\Pr(Y = j|X = x_0)\]</span> is largest. This very simple classifier is called the <em>Bayes classifier</em>. In a two-class problem the classification will be class one if <span class="math inline">\(Pr(Y = 1|X = x_0) &gt; 0.5\)</span>, and class two otherwise. The line where the probability is exactly 0.5 is called the <em>Bayes decision boundary</em>.</p>
<p>The Bayes classifier produces the lowest possible test error rate, called <em>the Bayes error rate</em> given by, <span class="math display">\[1 - E \big( \max_j \Pr(Y = j|X) \big ),\]</span> where the expectation averages the probability over all possible values of <span class="math inline">\(X\)</span>. The Bayes error rate is analogous to the irreducible error.</p>
<p>In theory we would always like to predict qualitative responses using the Bayes classifier. But for real data, we do not know the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, and so computing the Bayes classifier is impossible. Therefore, the Bayes classifier serves as an unattainable gold standard against which to compare other methods.</p>
<h3 id="k-nearest-neighbors">K-Nearest Neighbors</h3>
<p>Given a positive integer <span class="math inline">\(K\)</span> and a test observation <span class="math inline">\(x_0\)</span>, the K-nearest neighbors (KNN) classifier first identifies the K points in the training data that are closest to <span class="math inline">\(x_0\)</span>, represented by <span class="math inline">\(\mathcal N_0\)</span>. It then estimates the conditional probability for class j as the fraction of points in <span class="math inline">\(\mathcal N_0\)</span> whose response values equal j:</p>
<p><span class="math display">\[\Pr(Y = j|X = x_0) = \frac{1}{K} \sum_{i \in \mathcal N_0} I(y_i = j).\]</span></p>
<p>Finally, KNN applies Bayes rule and classifies the test observation <span class="math inline">\(x_0\)</span> to the class with the largest probability.</p>
<p>The choice of <span class="math inline">\(K\)</span> has a drastic effect on the KNN classifier obtained. As <span class="math inline">\(K\)</span> grows, the method becomes less flexible and produces a decision boundary that is close to linear. This corresponds to a low-variance but high-bias classifier. In general, as we use more flexible classification methods, the training error rate will decline but the test error rate may not.</p>
<p>In both the regression and classification settings, choosing the correct level of flexibility is critical to the success of any statistical learning method. The bias-variance tradeoff, and the resulting U-shape in the test error, can make this a difficult task.</p>
<p>Nearest Neighbors can be good when the number of variables, <span class="math inline">\(p\)</span>, is small, i.e. <span class="math inline">\(p \leq 4\)</span> and for a large number of sample points. It is one of many techniques called smoothers, like kernel and spline smoothing. Unfortunately this method is very poor when <span class="math inline">\(p\)</span> is large, since possible nearby neighbors tend to be far away in high dimensions. This is known as the curse of dimensionality. We need to get a reasonable fraction of the <span class="math inline">\(N\)</span> values of <span class="math inline">\(y_i\)</span> to average in order to bring the variance of our model down. However, as we increase the dimensions, the radius we need to search increases and we lose the efficacy of estimating using local averages</p>
<h1 id="linear-regression">Linear Regression</h1>
<p>Linear regression is a very simple approach for supervised learning and is a useful tool for predicting a quantitative response.</p>
<h2 id="simple-linear-regression">Simple Linear Regression</h2>
<p>Simple linear regression is a very straightforward approach for predicting a quantitative response <span class="math inline">\(Y\)</span> on the basis of a single predictor variable <span class="math inline">\(X\)</span>. It assumes that there is approximately a linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. We will sometimes describe this by saying that we are regressing <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span>.</p>
<p><span class="math display">\[Y \approx \beta_0 + \beta_1 X\]</span></p>
<p><span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are two unknown constants that represent the intercept and slope terms in the linear model and are known as its coefficients or parameters.</p>
<p>We may use our training data to produce estimates <span class="math inline">\(\hat \beta_0\)</span> and <span class="math inline">\(\hat \beta_1\)</span> for prediction.</p>
<p><span class="math display">\[\hat y  \approx \hat \beta_0 + \hat \beta_1 x\]</span></p>
<p>where <span class="math inline">\(\hat y\)</span> indicates a prediction of <span class="math inline">\(Y\)</span> on the basis of <span class="math inline">\(X = x\)</span>.</p>
<h3 id="estimating-the-coefficients">Estimating the Coefficients</h3>
<p>Let <span class="math inline">\((x_1, y_1), (x_2, y_2),..., (x_n, y_n)\)</span> represent <span class="math inline">\(n\)</span> observation pairs, each of which consists of a measurement of <span class="math inline">\(X\)</span> and a measurement of <span class="math inline">\(Y\)</span>.</p>
<p>We want to find an intercept <span class="math inline">\(\hat \beta_0\)</span> and a slope <span class="math inline">\(\hat \beta_1\)</span> such that the resulting line is as close as possible to the <span class="math inline">\(n\)</span> data points. The most common definition of closeness involves minimizing the least squares criterion.</p>
<p>Let <span class="math inline">\(\hat y_i = \hat \beta_0 + \hat \beta_1 x_i\)</span> be the prediction for <span class="math inline">\(Y\)</span> based on the <span class="math inline">\(i\)</span>th value of <span class="math inline">\(X\)</span>. Then <span class="math inline">\(e_i = y_i - \hat y_i\)</span> represents the <span class="math inline">\(i\)</span>th residual – this is the difference between <span class="math inline">\(i\)</span>th observed response and its prediction.</p>
<p>We define the <em>residual sum of squares (RSS)</em> as, <span class="math display">\[\begin{aligned}
     RSS &amp;= e_1^2 + e_2^2 + \dots + e_n^2\\
     &amp;=  (y_1 - \hat \beta_0 - \hat \beta_1 x_1)^2 + \cdots + 
     (y_n - \hat \beta_0 - \hat \beta_1 x_n)^2\end{aligned}\]</span></p>
<p>The least squares approach chooses <span class="math inline">\(\hat \beta_0\)</span> and <span class="math inline">\(\hat \beta_1\)</span> to minimize the RSS.</p>
<p>Let <span class="math inline">\(y \equiv \frac{1}{n} \sum^n_{i=1} y_i\)</span> and <span class="math inline">\(\bar x \equiv \frac{1}{n} \sum_{i=1}^n x_i\)</span> be the sample means. Then the the <em>least squares coefficient estimates</em> are given by,</p>
<p><span class="math display">\[\begin{aligned}
    \hat \beta_0 &amp;= \frac{\sum_{i=1}^n (x_i - \bar x) (y_i - \bar y)}{\sum_{i=1}^n (x_i - \bar x)^2} \\ 
    \hat \beta_1 &amp;= \bar y - \hat \beta_1 \bar x\end{aligned}\]</span></p>
<h3 id="assessing-the-accuracy-of-the-coefficient-estimates">Assessing the Accuracy of the Coefficient Estimates</h3>
<p>If <span class="math inline">\(f\)</span> is to be approximated by a linear function, then we can write this relationship as a <em>population regression line</em>,</p>
<p><span class="math display">\[\begin{aligned}
 Y &amp;= f(X) +  \epsilon \\
 Y  &amp;= \beta_0 +  \beta_1 X + \epsilon\end{aligned}\]</span></p>
<p>Where <span class="math inline">\(\beta_0\)</span> is the intercept, i.e. the expected value of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X = 0\)</span>, <span class="math inline">\(\beta_1\)</span> is the slope, i.e. the average increase in <span class="math inline">\(Y\)</span> in a unit of increase of <span class="math inline">\(X\)</span>, and <span class="math inline">\(\epsilon\)</span> is a catch-all error term.</p>
<p>The <em>least squares line</em> can always be computed using the coefficient estimates, however, the population regression line is unobserved. Fundamentally, the concept of these two lines is a natural extension of the standard statistical approach of using information from a sample to estimate characteristics of an unmeasured large population.</p>
<p>The analogy between linear regression and estimation of the mean of a random variable is an apt one based on the concept of bias. if we could average a huge number of estimates <span class="math inline">\(\hat \mu\)</span> of the true population mean <span class="math inline">\(\mu\)</span> obtained from a huge number of sets of observations, then this average would exactly equal <span class="math inline">\(\mu\)</span>. Thus it is an unbiased estimator, i.e. it does not systematically over- or under-estimate the true parameter.</p>
<p>To determine the accuracy of a sample mean <span class="math inline">\(\hat \mu\)</span> as an estimate of <span class="math inline">\(\mu\)</span>, we can find the <em>standard error</em>, which roughly tells us the average amount that this estimate differs from the actual value of <span class="math inline">\(\mu\)</span>.</p>
<p><span class="math display">\[\text{Var} (\hat \mu) = \text{SE} (\hat \mu ) ^2 = \frac{\sigma^2}{n},\]</span></p>
<p>where <span class="math inline">\(\sigma\)</span> is the standard deviation of each of the realizations <span class="math inline">\(y_i\)</span> of <span class="math inline">\(Y\)</span>.</p>
<p>To compute the standard errors associated with <span class="math inline">\(\hat \beta_0\)</span> and <span class="math inline">\(\hat \beta_1\)</span>,</p>
<p><span class="math display">\[\begin{aligned}
    \text{SE} ( \hat \beta_0 )^2 &amp;= \sigma^2 \big (  \frac{\bar x^2}{(x_i - \bar x)^2}  \big )\\ 
    \text{SE} ( \hat \beta_1 )^2 &amp;= \frac{\sigma^2}{\sum_{i=1}^n (x_i - \bar x)^2} \end{aligned}\]</span></p>
<p>where <span class="math inline">\(\sigma^2 = \text{Var}()\)</span>. In general, <span class="math inline">\(\sigma^2\)</span> is not known, but can be estimated from the data. The estimate of <span class="math inline">\(\sigma\)</span> is known as the <em>residual standard error</em>, and is given by the formula, <span class="math display">\[RSE = \sqrt{RSS/(n - 2)}\]</span></p>
<p>Standard errors can be used to compute <em>confidence intervals</em>. For example, a 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter.</p>
<p>For linear regression the confidence interval of the coefficients is given by, <span class="math display">\[\begin{aligned}
    \hat \beta_1 &amp;\pm 2 \cdot \text{SE}(\hat \beta_1)  \\
    \hat \beta_0 &amp;\pm 2 \cdot \text{SE}(\hat \beta_0).\end{aligned}\]</span></p>
<p>Standard errors can also be used to perform hypothesis tests on the coefficients. The null hypotheses – <span class="math inline">\(H_0\)</span> : There is no relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, i.e. <span class="math inline">\(\beta_1 = 0\)</span>. The alternative hypothesis – <span class="math inline">\(H_a\)</span> : There is some relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, i.e. <span class="math inline">\(\beta_1 \neq 0\)</span>. Setting <span class="math inline">\(\beta_1\)</span> removes the <span class="math inline">\(X\)</span> term in <span class="math inline">\(Y = \beta_0 + \beta_1 X + \epsilon\)</span>.</p>
<p>If <span class="math inline">\(\text{SE}(\hat \beta_1)\)</span> is small, then even relatively small values of <span class="math inline">\(\hat \beta_1\)</span> may provide strong evidence that <span class="math inline">\(\hat \beta_1 \neq 0\)</span> and hence that there is a relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The <em>t-statistic</em> given by, <span class="math display">\[t = \frac{\hat \beta_1 - 0}{\text{SE}(\hat \beta_1)}\]</span> measures the number of standard deviations that <span class="math inline">\(\hat \beta_1\)</span> is away from 0. If there is no relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the t-statistics will have a distribution with n - 2 degrees of freedom.</p>
<p>The <em>p-value</em> corresponds to the probability of observing any number equal to <span class="math inline">\(|t|\)</span> or larger in absolute value, assuming <span class="math inline">\(\beta_1= 0\)</span>. A small p-value indicates that it is unlikely to observe such a substantial association between the predictor and the response due to chance, in the absence of any real association, so we can infer that there is an association between the predictor and the response. Then, we can reject the null hypothesis and may declare a relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> if the p-value is small enough.</p>
<p>Simply put, a p-value is the probability that random chance generated the data, or (+) another event else that is equal or rarer. The traditional threshold for determining significance of a p-value is <span class="math inline">\(&lt; 0.05\)</span>.</p>
<h3 id="assessing-the-accuracy-of-the-model">Assessing the Accuracy of the Model</h3>
<p>Given the alternative hypothesis holds, we likely want to quantify the extent to which the model fits the data. A linear regression fit is typically assessed using two related quantities: the residual standard error (RSE) and the <span class="math inline">\(R^2\)</span> statistic.</p>
<h3 class="unnumbered" id="residual-error">Residual Error</h3>
<p>The RSE is an estimate of the standard deviation of the irreducible error term <span class="math inline">\(\epsilon\)</span>. Roughly speaking, it is the average amount that the response will deviate from the true regression line and is considered a measure of the lack of fit of the model to the data.</p>
<p>Recall, RSS is the residual sum of squares. Then the RSE is given by, <span class="math display">\[RSE = \sqrt{\frac{1}{(n-2)} \text{RSS} } 
    = \sqrt{\frac{1}{(n-2)} \sum_{i=1}^n (y_i - \hat y_i)^2 }.\]</span></p>
<h3 class="unnumbered" id="r2-statistic">R2 Statistic</h3>
<p>The <span class="math inline">\(R^2\)</span> statistic provides an alternative measure of fit in terms of the proportion of variance explained and so it always takes on a value between 0 and 1, and is independent of the scale of <span class="math inline">\(Y\)</span> like the RSE. The <span class="math inline">\(R^2\)</span> statistic is a measure of the linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>Let <span class="math inline">\(\text{TSS} = (y_i - \hat y)^2\)</span> be the <em>total sum of squares</em> and , RSS is the residual sum of squares. Then <span class="math inline">\(R^2\)</span> is given by, <span class="math display">\[R^2 =  \frac{\text{TSS} - \text{RSS}}{\text{TSS}} = 1 - \frac{RSS}{TSS}\]</span></p>
<p>TSS can be thought of as the amount of variability inherent in the response before the regression is performed while RSS measures the amount of variability that is left unexplained after performing the regression. TSS - RSS measures the amount of variability in the response that is explained (or removed) by performing the regression, and <span class="math inline">\(R^2\)</span> measures the proportion of variability in <span class="math inline">\(Y\)</span> that can be explained using <span class="math inline">\(X\)</span>.</p>
<p>An <span class="math inline">\(R^2\)</span> statistic that is close to 1 indicates that a large proportion of the variability in the response has been explained by the regression, while the converse holds for a value close to 0. The <span class="math inline">\(R^2\)</span> statistic is similar to correlation, but holds for a between a larger number of variables.</p>
<h2 id="multiple-linear-regression">Multiple Linear Regression</h2>
<p>We can handle multiple predictors by giving each predictor a separate slope coefficient in a single model. In general, suppose that we have <span class="math inline">\(p\)</span> distinct predictors. Then the multiple linear regression model takes the form,</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p + \epsilon,\]</span> where <span class="math inline">\(X_j\)</span> represents the <span class="math inline">\(j\)</span>th predictor and <span class="math inline">\(\beta_j\)</span> quantifies the association between that variable and the response, namely the average effect on <span class="math inline">\(Y\)</span> of a one unit increase in <span class="math inline">\(X_j\)</span> , holding all other predictors fixed.</p>
<h3 id="estimating-the-regression-coefficients">Estimating the Regression Coefficients</h3>
<p>Given estimates <span class="math inline">\(\hat \beta_0, \hat \beta_1,..., \hat \beta_p\)</span>, we can make predictions using the formula, <span class="math display">\[\hat y= \hat \beta_0, \hat \beta_1 x_1,..., \hat \beta_p x_p.\]</span> We choose <span class="math inline">\(\beta_0, \beta_1,...,\beta_p\)</span> to minimize the sum of squared residuals, <span class="math display">\[\begin{aligned}
    RSS &amp;= \sum_{i=1}^{n} (y_i - \hat y_i)^2 \\
    &amp;=  \sum_{i=1}^{n}(y_i - \hat \beta_0 - \hat \beta_1 x_1,..., \hat \beta_p x_p)^2.\end{aligned}\]</span> The values <span class="math inline">\(\hat \beta_0, \hat \beta_1 x_1,..., \hat \beta_p x_p\)</span> that minimize the above equation are the multiple least squares regression coefficient estimates. These values are most easily represented using matrix algebra and will be revisited later.</p>
<h3 id="some-important-questions">Some Important Questions</h3>
<ol type="1">
<li><p>Is at least one of the predictors <span class="math inline">\(X_1, X_2,...,X_p\)</span> useful in predicting the response?</p></li>
<li><p>Do all the predictors help to explain <span class="math inline">\(Y\)</span>, or is only a subset of the predictors useful?</p></li>
<li><p>How well does the model fit the data?</p></li>
<li><p>Given a set of predictor values, what response value should we predict, and how accurate is our prediction?</p></li>
</ol>
<h3 class="unnumbered" id="is-there-a-relationship-between-the-response-and-predictors">Is There a Relationship Between the Response and Predictors?</h3>
<p>In the multiple regression setting with <span class="math inline">\(p\)</span> predictors, we need to ask whether all of the regression coefficients are zero. The null hypothesis becomes, <span class="math display">\[H_0 : \beta_1 = \beta_2 = \dots = \beta_p = 0.\]</span></p>
<p>This hypothesis test is performed by computing the <em>F-statistic</em>. <span class="math display">\[F = \frac{(\text{TSS} - \text{RSS})/p }{\text{RSS}/ (n-p-1)}\]</span></p>
<p>When there is no relationship between the response and predictors, one would expect the F-statistic to take on a value close to 1. On the other hand, if <span class="math inline">\(H_a\)</span> is true (i.e. at least one <span class="math inline">\(\beta_j\)</span> is non-zero.), then <span class="math inline">\(E{(\text{TSS} - \text{RSS})/p} &gt; \sigma^2\)</span>, so we expect <span class="math inline">\(F\)</span> to be greater than <span class="math inline">\(1\)</span>.</p>
<p>The approach of using an F-statistic to test for any association between the predictors and the response works when <span class="math inline">\(p\)</span> is relatively small, and certainly small compared to <span class="math inline">\(n\)</span>. However, sometimes we have a very large number of variables. If <span class="math inline">\(p&gt;n\)</span> then there are more coefficients <span class="math inline">\(\beta_j\)</span> to estimate than observations from which to estimate them. In this case we cannot even fit the multiple linear regression model using least squares, so the F-statistic cannot be used. Approaches like forward selection, discussed later, can be used instead.</p>
<h3 id="deciding-on-important-variables">Deciding on Important Variables</h3>
<p>The first step in a multiple regression analysis is to compute the F-statistic and to examine the associated p-value. If we conclude on the basis of the p-value that at least one of the predictors is related to the response, then it is natural to wonder which are the relevant ones. The task of determining which predictors are associated with the response, in order to fit a single model involving only those predictors, is referred to as <em>variable selection</em>, discussed more in detail in chapter 6.</p>
<p>There are a total of <span class="math inline">\(2^p\)</span> models that contain subsets of <span class="math inline">\(p\)</span> variables. . In general, we cannot consider all <span class="math inline">\(2^p\)</span> models, and instead we need an automated and efficient approach to choose a smaller set of models to consider. There are three classical approaches for this task:</p>
<ol type="1">
<li><p><em>Forward selection</em> – We begin with the null model, i.e. a model that contains an intercept but no predictors. We then fit p simple linear regressions and add to the null model the variable that results in the lowest RSS. We then add to that model the variable that results in the lowest RSS for the new two-variable model. This approach is continued until some stopping rule is satisfied.</p></li>
<li><p><em>Backward selection</em> – We start with all variables in the model, and remove the variable with the largest p-value—that is, the variable that is the least statistically significant. The new (<span class="math inline">\(p - 1\)</span>)-variable model is fit, and the variable with the largest p-value is removed. This procedure continues until a stopping rule is reached, i.e. when all remaining variables have a p-value below some threshold.</p></li>
<li><p>Mixed selection – This is a combination of forward and backward selection. We start with no variables in the model, and as with forward selection, we add the variable that provides the best fit. Since the p-values for variables can become larger as new predictors are added to the model, if at any point the p-value for one of the variables in the model rises above a certain threshold, then we remove that variable from the model. We continue to perform these forward and backward steps until all variables in the model have a sufficiently low p-value.</p></li>
</ol>
<p>Backward selection cannot be used if <span class="math inline">\(p&gt;n\)</span>, while forward selection can always be used. Forward selection is a greedy approach, and might include variables early that later become redundant. Mixed selection can remedy this.</p>
<h3 class="unnumbered" id="model-fit">Model Fit</h3>
<p>In multiple linear regression, <span class="math inline">\(R^2 = Cor(Y, \hat Y)^2\)</span>, the square of the correlation between the response and the fitted linear model. <span class="math inline">\(R^2\)</span> will always increase when more variables are added to the model, even if they are only weakly associated with the response. This is due to the fact that adding another variable to the least squares equations must allow us to fit the training data more accurately, so the R2 statistic, which is also computed on the training data, must increase.</p>
<h3 class="unnumbered" id="predictions">Predictions</h3>
<p>After fitting a regression model, there are three sorts of uncertainty associated with a prediction.</p>
<ol type="1">
<li><p>There will be some inaccuracy in the coefficient estimates related to the reducible error of using the least squares plane. We can compute a confidence interval in order to determine how close <span class="math inline">\(\hat Y\)</span> will be to <span class="math inline">\(f(X)\)</span>.</p></li>
<li><p>There is potentially an additional source of reducible error, called <em>model bias</em>, from using a linear model to represent complex non-linear data.</p></li>
<li><p>The will be some amount of irreducible or random error in the model. <em>Prediction intervals</em> are used to incorporate both the error in the estimate for <span class="math inline">\(f(X)\)</span> (the reducible error) and the uncertainty as to how much an individual point will differ from the population regression plane (the irreducible error).</p></li>
</ol>
<h2 id="other-considerations-in-the-regression-model">Other Considerations in the Regression Model</h2>
<h3 id="qualitative-predictors">Qualitative Predictors</h3>
<h3 class="unnumbered" id="predictors-with-only-two-levels">Predictors with Only Two Levels</h3>
<p>If a qualitative predictor (also known as a factor) only has two levels, or possible values, then incorporating it into a regression model is very simple. We simply create an indicator or dummy variable that takes on two possible numerical values, i.e. (0 or 1) and use this variable as a predictor in the regression equation.</p>
<h3 class="unnumbered" id="qualitative-predictors-with-more-than-two-levels">Qualitative Predictors with More than Two Levels</h3>
<p>In this situation, we can create additional dummy variables. There will always be one fewer dummy variable than the number of levels. The level with no dummy variable is known as the <em>baseline</em>.</p>
<h3 id="extensions-of-the-linear-model">Extensions of the Linear Model</h3>
<p>Two of the most impactful assumptions in the standard linear regression model state that the relationship between the predictors and response are additive and linear. The <em>additive</em> assumption means that the effect of changes in a predictor <span class="math inline">\(X_j\)</span> on the response Y is independent of the values of the other predictors. The <em>linear</em> assumption states that the change in the response <span class="math inline">\(Y\)</span> due to a one-unit change in <span class="math inline">\(X_j\)</span> is constant, regardless of the value of <span class="math inline">\(X_j\)</span>.</p>
<h3 class="unnumbered" id="removing-the-additive-assumption">Removing the Additive Assumption</h3>
<p>One way of extending this model to allow for interaction effects is to include a third predictor, called an interaction term, which is constructed by computing the product of X1 and X2. <span class="math display">\[\begin{aligned}
    Y &amp;= \beta_0 + \beta_1 X_ 1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \epsilon \\
    &amp;= \beta_0 + (\beta_1 + \beta_3 X_2)X_1 + \beta_2 X_2 + \epsilon \\
    &amp;= \beta_0 + \Tilde \beta_1 X_1 + \beta_2 X_2 + \epsilon\end{aligned}\]</span> Then, adjusting <span class="math inline">\(X_2\)</span> will change the impact of <span class="math inline">\(X_1\)</span> on <span class="math inline">\(Y\)</span>.</p>
<p>The <em>hierarchical principle</em> states that if we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant.</p>
<h3 class="unnumbered" id="non-linear-relationships">Non-linear Relationships</h3>
<p>In some cases, the true relationship between the response and the predictors may be nonlinear. A simple way to directly extend the linear model to accommodate non-linear relationships is to use <em>polynomial regression</em>, in which we include polynomial functions of the predictors in the regression model.</p>
<h3 id="potential-problems">Potential Problems</h3>
<ol type="1">
<li><p>Non-linearity of the response-predictor relationships.</p>
<p>Residual plots are useful graphical tool for identifying non-linearity. If there is a strong pattern in the plot of the residuals, <span class="math inline">\(e_i = y_i - \hat y_i\)</span>, versus the predictor <span class="math inline">\(x_i\)</span>, there may be a problem with some aspect of the linear mode.</p></li>
<li><p>Correlation of error terms.</p>
<p>In general, the assumption of uncorrelated errors is extremely important for linear regression as well as for other statistical methods.</p>
<p>If there is correlation among the error terms, then the estimated standard errors will tend to underestimate the true standard errors, confidence and prediction intervals will be narrower than they should be, and p-values associated with the model will be lower than they should be. Such correlations frequently occur in the context of time series data.</p>
<p>We can plot the residuals from our model as a function of time and if the errors are uncorrelated, then there should be no discernible pattern. Otherwise we may find that <em>tracking</em> exists, i.e. adjacent residuals often have similar values.</p></li>
<li><p>Non-constant variance of error terms.</p>
<p>An important assumption is that error terms have constant variance, i.e. <span class="math inline">\(\text{Var}(i) = \sigma^2\)</span>. One can identify non-constant variance, a.k.a. heteroscedasticity, from the presence of a funnel shape in the residual plot.</p>
<p>A simple remedy may be to fit our model by <em>weighted least squares</em>, with weights proportional to the inverse weighted variances.</p></li>
<li><p>Outliers.</p>
<p>A point for which <span class="math inline">\(y_i\)</span> is far from the value predicted by the model can occur for many reasons, possibly as a result of incorrect recording of an observation. It is typical for an outlier that does not have an unusual predictor value to have little effect on the least squares fit, but may have an effect on the RSE, used to compute all confidence intervals or p-values, as well as <span class="math inline">\(R^2\)</span>.</p>
<p>Residual plots can be used to identify outliers, but in practice, it can be difficult to decide how large a residual needs to be before we consider the point to be an outlier. Instead, we can plot the <em>studentized residuals</em>, computed by dividing each residual <span class="math inline">\(e_i\)</span> by its estimated standard error. Observations whose studentized residuals are greater than 3 in absolute value are possible outliers and can be removed or examined to determine a deficiency with the model, such as a missing predictor.</p></li>
<li><p>High-leverage points.</p>
<p>In contrast to outliers, observations with <em>high leverage</em> have an unusual value for <span class="math inline">\(x_i\)</span>. Removing a high leverage observation has a much more substantial impact on the least squares line than removing the outlier.</p>
<p>To quantify an observation’s leverage, we compute the <em>leverage statistic</em>. The leverage statistic <span class="math inline">\(h_i\)</span> is always between <span class="math inline">\(1/n\)</span> and <span class="math inline">\(1\)</span>, and the average leverage for all the observations is always equal to <span class="math inline">\((p + 1)/n\)</span>. For a simple linear regression, <span class="math display">\[h_i =  \frac{1}{n} \frac{(x_i - \bar x)^2}{\sum_{i&#39; = 1}^n (x_i - \bar x)^2}\]</span></p></li>
<li><p>Collinearity.</p>
<p><em>Collinearity</em> refers to the situation in which two or more predictor variables are closely related to one another. In the regression context, it can be difficult to separate out the individual effects of collinear variables on the response and reduces the accuracy of the estimates of the regression coefficients. This causes the standard error for <span class="math inline">\(\hat \beta_j\)</span> to grow and the t-statistic to decline which in turn means the power of the hypothesis test - the probability of correctly detecting a non-zero coefficient - is reduced by collinearity.</p>
<p>A simple way to detect collinearity is to look at the correlation matrix of the predictors, though this may not always work. In particular, it is possible for collinearity to exist between three or more variables in what’s called <em>multicollinearity</em>. In this situation it’s better to compute the <em>variance inflation factor</em> (VIF), the ratio of the variance of <span class="math inline">\(\hat \beta_j\)</span> when fitting the full model divided by the variance of <span class="math inline">\(\hat \beta_j\)</span> if fit on its own. A VIF value of 1indicates the complete absence of collinearity, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity.</p>
<p><span class="math display">\[VIF(\hat \beta_j ) = \frac{1}{1-R^2_{X_j|X_{-j}}}\]</span> where <span class="math inline">\(R^2_{X_j|X_{-j}}\)</span> is the <span class="math inline">\(R^2\)</span> from a regression of <span class="math inline">\(X_j\)</span> onto all of the other predictors.</p>
<p>To mitigate collinerarity, we may drop one of the problematic variables from the regression, which will also reduce redundancy. Alternatively, we may combine the collinear variables together into a single predictor.</p>
<h2 id="comparison-of-linear-regression-with-k-nearest-neighbors">Comparison of Linear Regression with K-Nearest Neighbors</h2>
<p>Linear regression is an example of a parametric approach and has many advantages: they are often easy to fit, the coefficients have simple interpretations, and tests of statistical significance can be easily performed. But parametric methods do have a disadvantage: by construction, they make strong assumptions about the form of <span class="math inline">\(f(X)\)</span>.</p>
<p>In contrast, non-parametric methods do not explicitly assume a parametric form for <span class="math inline">\(f(X)\)</span>, and thereby provide an alternative and more flexible approach for performing regression.</p>
<p>One of the simplest and best-known non-parametric methods is <em>K-nearest neighbors regression</em> (KNN regression), which is closely related to the KNN classifier.</p>
<p>Given a value for <span class="math inline">\(K\)</span> and a prediction point <span class="math inline">\(x_0\)</span>, KNN regression first identifies the <span class="math inline">\(K\)</span> training observations that are closest to <span class="math inline">\(x_0\)</span>, represented by <span class="math inline">\(\mathcal N_0\)</span>. It then estimates <span class="math inline">\(f(x_0)\)</span> using the average of all the training responses in <span class="math inline">\(mathcal N_0\)</span>. In general, the optimal value for K will depend on the bias-variance tradeoff.</p>
<p><span class="math display">\[\hat f (x_0) = \frac{1}{K} \sum_{x_i \in \mathcal N_0} y_i.\]</span></p>
<p>The parametric approach will outperform the nonparametric approach if the parametric form that has been selected is close to the true form of <span class="math inline">\(f\)</span>. Even when the true relationship is highly non-linear, KNN may still provide inferior results to linear regression. A decrease in performance as the dimension increases is a common problem for KNN, and results from the fact that in higher dimensions there is effectively a reduction in sample size. As a general rule, parametric methods will tend to outperform non-parametric approaches when there is a small number of observations per predictor.</p>
<h1 id="classification">Classification</h1>
<h1 id="resampling-methods">Resampling Methods</h1>
<h1 id="linear-model-selection-and-regularization">Linear Model Selection and Regularization</h1>
<h1 id="moving-beyond-linearity">Moving Beyond Linearity</h1>
<h1 id="tree-based-methods">Tree-based Methods</h1>
<h1 id="support-vector-machines">Support Vector Machines</h1>
<h1 id="unsupervised-learning">Unsupervised Learning</h1></li>
</ol>
<p>Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. An Introduction to Statistical Learning : with Applications in R. New York :Springer, 2013.</p>
<p>http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf</p>
<p>https://arxiv.org/pdf/1010.3162.pdf</p>
<p>http://bactra.org/notebooks/learning-theory.html</p>
</body>
</html>
